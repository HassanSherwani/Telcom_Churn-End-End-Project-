{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Nueral network using Keras library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To evaluate\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working on model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('clean_feature.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1200, 28)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Months.customer</th>\n",
       "      <th>Minutes.in.2018</th>\n",
       "      <th>Number.of.SMS</th>\n",
       "      <th>KBs.used</th>\n",
       "      <th>Total.Unique.Calls</th>\n",
       "      <th>Churn.Status</th>\n",
       "      <th>Customer.Age</th>\n",
       "      <th>LoginsSite.Last.Month</th>\n",
       "      <th>EndSubscription</th>\n",
       "      <th>Total.Call.centre.complaint.calls</th>\n",
       "      <th>...</th>\n",
       "      <th>Province_Overijssel</th>\n",
       "      <th>Province_Utrecht</th>\n",
       "      <th>Province_Zeeland</th>\n",
       "      <th>Province_Zuid-Holland</th>\n",
       "      <th>Previous.provider_Ben</th>\n",
       "      <th>Previous.provider_KPN</th>\n",
       "      <th>Previous.provider_T-Mobile</th>\n",
       "      <th>Previous.provider_TELE2</th>\n",
       "      <th>Previous.provider_Telfort</th>\n",
       "      <th>Previous.provider_Vodafone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>126.83</td>\n",
       "      <td>4091.6160</td>\n",
       "      <td>81</td>\n",
       "      <td>3.624375e+03</td>\n",
       "      <td>117</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96.83</td>\n",
       "      <td>3179.2800</td>\n",
       "      <td>101</td>\n",
       "      <td>5.518428e+05</td>\n",
       "      <td>106</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>127.30</td>\n",
       "      <td>7233.1824</td>\n",
       "      <td>79</td>\n",
       "      <td>2.128473e+06</td>\n",
       "      <td>94</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>121.43</td>\n",
       "      <td>7336.1760</td>\n",
       "      <td>121</td>\n",
       "      <td>2.180738e+04</td>\n",
       "      <td>450</td>\n",
       "      <td>1</td>\n",
       "      <td>54</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>126.97</td>\n",
       "      <td>5390.9400</td>\n",
       "      <td>16</td>\n",
       "      <td>4.648857e+05</td>\n",
       "      <td>308</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Months.customer  Minutes.in.2018  Number.of.SMS      KBs.used  \\\n",
       "0           126.83        4091.6160             81  3.624375e+03   \n",
       "1            96.83        3179.2800            101  5.518428e+05   \n",
       "2           127.30        7233.1824             79  2.128473e+06   \n",
       "3           121.43        7336.1760            121  2.180738e+04   \n",
       "4           126.97        5390.9400             16  4.648857e+05   \n",
       "\n",
       "   Total.Unique.Calls  Churn.Status  Customer.Age  LoginsSite.Last.Month  \\\n",
       "0                 117             0            44                      6   \n",
       "1                 106             0            31                      8   \n",
       "2                  94             0            35                      6   \n",
       "3                 450             1            54                      5   \n",
       "4                 308             0            53                      7   \n",
       "\n",
       "   EndSubscription  Total.Call.centre.complaint.calls  \\\n",
       "0                5                                  2   \n",
       "1                4                                  2   \n",
       "2                5                                  3   \n",
       "3                3                                  1   \n",
       "4                3                                  2   \n",
       "\n",
       "              ...              Province_Overijssel  Province_Utrecht  \\\n",
       "0             ...                                0                 1   \n",
       "1             ...                                0                 0   \n",
       "2             ...                                0                 0   \n",
       "3             ...                                0                 0   \n",
       "4             ...                                0                 0   \n",
       "\n",
       "   Province_Zeeland  Province_Zuid-Holland  Previous.provider_Ben  \\\n",
       "0                 0                      0                      0   \n",
       "1                 0                      0                      0   \n",
       "2                 0                      0                      0   \n",
       "3                 0                      0                      1   \n",
       "4                 0                      0                      0   \n",
       "\n",
       "   Previous.provider_KPN  Previous.provider_T-Mobile  Previous.provider_TELE2  \\\n",
       "0                      1                           0                        0   \n",
       "1                      0                           0                        0   \n",
       "2                      0                           0                        1   \n",
       "3                      0                           0                        0   \n",
       "4                      0                           0                        1   \n",
       "\n",
       "   Previous.provider_Telfort  Previous.provider_Vodafone  \n",
       "0                          0                           0  \n",
       "1                          1                           0  \n",
       "2                          0                           0  \n",
       "3                          0                           0  \n",
       "4                          0                           0  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking rows now\n",
    "def summary_missing(dataset):\n",
    "    n_miss = dataset.isnull().sum()\n",
    "    n_obs = dataset.shape[0]\n",
    "    n_miss_per = n_miss/n_obs*100\n",
    "    n_miss_tbl = pd.concat([n_miss, n_miss_per], axis = 1).sort_values(1, ascending = False).round(1)\n",
    "    n_miss_tbl = n_miss_tbl[n_miss_tbl[1] != 0]\n",
    "    print('No. of fields: ', dataset.shape[0])\n",
    "    print('No. of missing fields: ', n_miss_tbl.shape[0])\n",
    "    n_miss_tbl = n_miss_tbl.rename(columns = {0:'No. of mising Value', 1:'%age of missing Value'})\n",
    "    return n_miss_tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of fields:  1200\n",
      "No. of missing fields:  0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No. of mising Value</th>\n",
       "      <th>%age of missing Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [No. of mising Value, %age of missing Value]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_missing(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Separate features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df['Churn.Status'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = df.drop(['Churn.Status'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hassan\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Hassan\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X = StandardScaler().fit_transform(df_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.81233973, -0.08501568,  0.84931911, -0.31303268, -0.2806863 ,\n",
       "        -0.30603414,  0.111791  ,  0.22734047,  0.03398389,  1.22363692,\n",
       "        -0.18328047, -0.17332893, -0.37652323, -0.15171652, -0.25264558,\n",
       "        -0.40486853, -0.42965125, -0.25819889,  3.5331496 , -0.17586311,\n",
       "        -0.56066773, -0.40765274,  1.83835852, -0.35156152, -0.55938384,\n",
       "        -0.33178797, -0.41733475],\n",
       "       [ 1.12294446, -0.21213609,  1.18043229, -0.23401078, -0.31734247,\n",
       "        -0.94123007,  1.14370789, -0.18916115,  0.03398389, -0.21323965,\n",
       "        -0.18328047, -0.17332893, -0.37652323, -0.15171652, -0.25264558,\n",
       "        -0.40486853, -0.42965125, -0.25819889, -0.28303359, -0.17586311,\n",
       "        -0.56066773, -0.40765274, -0.54396354, -0.35156152, -0.55938384,\n",
       "         3.01397306, -0.41733475]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1200, 27)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test-train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1660, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model, add dense layers one by one specifying activation function\n",
    "model = Sequential()\n",
    "model.add(Dense(15, input_dim=X_train.shape[1], activation='relu')) # input layer requires input_dim param\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dropout(.2))\n",
    "model.add(Dense(1, activation='sigmoid')) # sigmoid instead of relu for final probability between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model, adam gradient descent (optimized)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 15)                420       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                160       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 88        \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 677\n",
      "Trainable params: 677\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 200 samples\n",
      "Epoch 1/1000\n",
      "1000/1000 [==============================] - 1s 525us/step - loss: 0.7067 - acc: 0.4950 - val_loss: 0.7016 - val_acc: 0.4650\n",
      "Epoch 2/1000\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 0.6919 - acc: 0.5150 - val_loss: 0.6936 - val_acc: 0.5050\n",
      "Epoch 3/1000\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.6838 - acc: 0.5720 - val_loss: 0.6898 - val_acc: 0.5300\n",
      "Epoch 4/1000\n",
      "1000/1000 [==============================] - 0s 173us/step - loss: 0.6713 - acc: 0.6110 - val_loss: 0.6825 - val_acc: 0.5500\n",
      "Epoch 5/1000\n",
      "1000/1000 [==============================] - 0s 74us/step - loss: 0.6566 - acc: 0.6500 - val_loss: 0.6733 - val_acc: 0.5700\n",
      "Epoch 6/1000\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 0.6397 - acc: 0.6840 - val_loss: 0.6650 - val_acc: 0.5850\n",
      "Epoch 7/1000\n",
      "1000/1000 [==============================] - 0s 77us/step - loss: 0.6181 - acc: 0.6920 - val_loss: 0.6569 - val_acc: 0.5600\n",
      "Epoch 8/1000\n",
      "1000/1000 [==============================] - 0s 81us/step - loss: 0.5941 - acc: 0.7080 - val_loss: 0.6539 - val_acc: 0.5800\n",
      "Epoch 9/1000\n",
      "1000/1000 [==============================] - 0s 78us/step - loss: 0.5787 - acc: 0.7080 - val_loss: 0.6472 - val_acc: 0.6100\n",
      "Epoch 10/1000\n",
      "1000/1000 [==============================] - 0s 76us/step - loss: 0.5608 - acc: 0.7420 - val_loss: 0.6505 - val_acc: 0.5950\n",
      "Epoch 11/1000\n",
      "1000/1000 [==============================] - 0s 73us/step - loss: 0.5658 - acc: 0.7270 - val_loss: 0.6496 - val_acc: 0.6200\n",
      "Epoch 12/1000\n",
      "1000/1000 [==============================] - 0s 74us/step - loss: 0.5512 - acc: 0.7370 - val_loss: 0.6498 - val_acc: 0.6300\n",
      "Epoch 13/1000\n",
      "1000/1000 [==============================] - 0s 72us/step - loss: 0.5432 - acc: 0.7390 - val_loss: 0.6465 - val_acc: 0.6250\n",
      "Epoch 14/1000\n",
      "1000/1000 [==============================] - 0s 86us/step - loss: 0.5359 - acc: 0.7460 - val_loss: 0.6545 - val_acc: 0.6400\n",
      "Epoch 15/1000\n",
      "1000/1000 [==============================] - 0s 73us/step - loss: 0.5346 - acc: 0.7460 - val_loss: 0.6497 - val_acc: 0.6350\n",
      "Epoch 16/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.5260 - acc: 0.7530 - val_loss: 0.6546 - val_acc: 0.6400\n",
      "Epoch 17/1000\n",
      "1000/1000 [==============================] - 0s 68us/step - loss: 0.5216 - acc: 0.7560 - val_loss: 0.6520 - val_acc: 0.6400\n",
      "Epoch 18/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.5137 - acc: 0.7660 - val_loss: 0.6570 - val_acc: 0.6350\n",
      "Epoch 19/1000\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 0.5049 - acc: 0.7670 - val_loss: 0.6541 - val_acc: 0.6300\n",
      "Epoch 20/1000\n",
      "1000/1000 [==============================] - 0s 77us/step - loss: 0.5099 - acc: 0.7680 - val_loss: 0.6577 - val_acc: 0.6400\n",
      "Epoch 21/1000\n",
      "1000/1000 [==============================] - 0s 71us/step - loss: 0.4941 - acc: 0.7790 - val_loss: 0.6604 - val_acc: 0.6400\n",
      "Epoch 22/1000\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 0.4897 - acc: 0.7930 - val_loss: 0.6669 - val_acc: 0.6300\n",
      "Epoch 23/1000\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 0.4914 - acc: 0.7830 - val_loss: 0.6633 - val_acc: 0.6400\n",
      "Epoch 24/1000\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.4893 - acc: 0.7920 - val_loss: 0.6641 - val_acc: 0.6350\n",
      "Epoch 25/1000\n",
      "1000/1000 [==============================] - 0s 68us/step - loss: 0.4925 - acc: 0.7720 - val_loss: 0.6668 - val_acc: 0.6350\n",
      "Epoch 26/1000\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 0.4802 - acc: 0.7920 - val_loss: 0.6868 - val_acc: 0.6200\n",
      "Epoch 27/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.4700 - acc: 0.7980 - val_loss: 0.6798 - val_acc: 0.6350\n",
      "Epoch 28/1000\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 0.4607 - acc: 0.8020 - val_loss: 0.6736 - val_acc: 0.6300\n",
      "Epoch 29/1000\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.4582 - acc: 0.8070 - val_loss: 0.6782 - val_acc: 0.6200\n",
      "Epoch 30/1000\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.4452 - acc: 0.8130 - val_loss: 0.6979 - val_acc: 0.6200\n",
      "Epoch 31/1000\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 0.4263 - acc: 0.8120 - val_loss: 0.6922 - val_acc: 0.6350\n",
      "Epoch 32/1000\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.4487 - acc: 0.8100 - val_loss: 0.7023 - val_acc: 0.6250\n",
      "Epoch 33/1000\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 0.4448 - acc: 0.8120 - val_loss: 0.7039 - val_acc: 0.6250\n",
      "Epoch 34/1000\n",
      "1000/1000 [==============================] - 0s 68us/step - loss: 0.4478 - acc: 0.8250 - val_loss: 0.7056 - val_acc: 0.6200\n",
      "Epoch 35/1000\n",
      "1000/1000 [==============================] - 0s 68us/step - loss: 0.4335 - acc: 0.8130 - val_loss: 0.7009 - val_acc: 0.6350\n",
      "Epoch 36/1000\n",
      "1000/1000 [==============================] - 0s 76us/step - loss: 0.4384 - acc: 0.8130 - val_loss: 0.7119 - val_acc: 0.6300\n",
      "Epoch 37/1000\n",
      "1000/1000 [==============================] - 0s 139us/step - loss: 0.4272 - acc: 0.8150 - val_loss: 0.7138 - val_acc: 0.6350\n",
      "Epoch 38/1000\n",
      "1000/1000 [==============================] - 0s 74us/step - loss: 0.4251 - acc: 0.8300 - val_loss: 0.7239 - val_acc: 0.6350\n",
      "Epoch 39/1000\n",
      "1000/1000 [==============================] - 0s 69us/step - loss: 0.4293 - acc: 0.8250 - val_loss: 0.7196 - val_acc: 0.6400\n",
      "Epoch 40/1000\n",
      "1000/1000 [==============================] - 0s 77us/step - loss: 0.4019 - acc: 0.8260 - val_loss: 0.7325 - val_acc: 0.6600\n",
      "Epoch 41/1000\n",
      "1000/1000 [==============================] - 0s 71us/step - loss: 0.4130 - acc: 0.8340 - val_loss: 0.7417 - val_acc: 0.6500\n",
      "Epoch 42/1000\n",
      "1000/1000 [==============================] - 0s 70us/step - loss: 0.4096 - acc: 0.8240 - val_loss: 0.7499 - val_acc: 0.6500\n",
      "Epoch 43/1000\n",
      "1000/1000 [==============================] - 0s 80us/step - loss: 0.4066 - acc: 0.8250 - val_loss: 0.7381 - val_acc: 0.6650\n",
      "Epoch 44/1000\n",
      "1000/1000 [==============================] - 0s 71us/step - loss: 0.4039 - acc: 0.8350 - val_loss: 0.7427 - val_acc: 0.6550\n",
      "Epoch 45/1000\n",
      "1000/1000 [==============================] - 0s 71us/step - loss: 0.4001 - acc: 0.8260 - val_loss: 0.7532 - val_acc: 0.6650\n",
      "Epoch 46/1000\n",
      "1000/1000 [==============================] - 0s 69us/step - loss: 0.3938 - acc: 0.8400 - val_loss: 0.7572 - val_acc: 0.6600\n",
      "Epoch 47/1000\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.4834 - acc: 0.700 - 0s 51us/step - loss: 0.3892 - acc: 0.8360 - val_loss: 0.7847 - val_acc: 0.6550\n",
      "Epoch 48/1000\n",
      "1000/1000 [==============================] - 0s 70us/step - loss: 0.3833 - acc: 0.8430 - val_loss: 0.7768 - val_acc: 0.6600\n",
      "Epoch 49/1000\n",
      "1000/1000 [==============================] - 0s 69us/step - loss: 0.3862 - acc: 0.8410 - val_loss: 0.7732 - val_acc: 0.6550\n",
      "Epoch 50/1000\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.3725 - acc: 0.8430 - val_loss: 0.7926 - val_acc: 0.6600\n",
      "Epoch 51/1000\n",
      "1000/1000 [==============================] - 0s 70us/step - loss: 0.3802 - acc: 0.8500 - val_loss: 0.7918 - val_acc: 0.6450\n",
      "Epoch 52/1000\n",
      "1000/1000 [==============================] - 0s 74us/step - loss: 0.3668 - acc: 0.8550 - val_loss: 0.8002 - val_acc: 0.6500\n",
      "Epoch 53/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.3652 - acc: 0.8440 - val_loss: 0.8049 - val_acc: 0.6400\n",
      "Epoch 54/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.3635 - acc: 0.8440 - val_loss: 0.8211 - val_acc: 0.6450\n",
      "Epoch 55/1000\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.3543 - acc: 0.8590 - val_loss: 0.8139 - val_acc: 0.6600\n",
      "Epoch 56/1000\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.3572 - acc: 0.8510 - val_loss: 0.8200 - val_acc: 0.6700\n",
      "Epoch 57/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.3488 - acc: 0.8600 - val_loss: 0.8326 - val_acc: 0.6800\n",
      "Epoch 58/1000\n",
      "1000/1000 [==============================] - 0s 76us/step - loss: 0.3412 - acc: 0.8590 - val_loss: 0.8392 - val_acc: 0.6650\n",
      "Epoch 59/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.3549 - acc: 0.8500 - val_loss: 0.8503 - val_acc: 0.6550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/1000\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 0.3554 - acc: 0.8570 - val_loss: 0.8442 - val_acc: 0.6550\n",
      "Epoch 61/1000\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 0.3485 - acc: 0.8550 - val_loss: 0.8516 - val_acc: 0.6500\n",
      "Epoch 62/1000\n",
      "1000/1000 [==============================] - 0s 76us/step - loss: 0.3393 - acc: 0.8660 - val_loss: 0.8192 - val_acc: 0.6800\n",
      "Epoch 63/1000\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 0.3419 - acc: 0.8470 - val_loss: 0.8682 - val_acc: 0.6700\n",
      "Epoch 64/1000\n",
      "1000/1000 [==============================] - 0s 71us/step - loss: 0.3369 - acc: 0.8630 - val_loss: 0.8593 - val_acc: 0.6800\n",
      "Epoch 65/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.3324 - acc: 0.8630 - val_loss: 0.8645 - val_acc: 0.6650\n",
      "Epoch 66/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.3267 - acc: 0.8670 - val_loss: 0.8625 - val_acc: 0.6650\n",
      "Epoch 67/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.3269 - acc: 0.8640 - val_loss: 0.8869 - val_acc: 0.6600\n",
      "Epoch 68/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.3243 - acc: 0.8650 - val_loss: 0.8961 - val_acc: 0.6650\n",
      "Epoch 69/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.3234 - acc: 0.8710 - val_loss: 0.8880 - val_acc: 0.6700\n",
      "Epoch 70/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.3191 - acc: 0.8650 - val_loss: 0.9080 - val_acc: 0.6750\n",
      "Epoch 71/1000\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.3210 - acc: 0.8690 - val_loss: 0.9093 - val_acc: 0.6800\n",
      "Epoch 72/1000\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.3083 - acc: 0.8790 - val_loss: 0.9022 - val_acc: 0.6700\n",
      "Epoch 73/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.3002 - acc: 0.8730 - val_loss: 0.9422 - val_acc: 0.6650\n",
      "Epoch 74/1000\n",
      "1000/1000 [==============================] - 0s 58us/step - loss: 0.2970 - acc: 0.8840 - val_loss: 0.9564 - val_acc: 0.6650\n",
      "Epoch 75/1000\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.2997 - acc: 0.8800 - val_loss: 0.9732 - val_acc: 0.6600\n",
      "Epoch 76/1000\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 0.2979 - acc: 0.8860 - val_loss: 0.9715 - val_acc: 0.6650\n",
      "Epoch 77/1000\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.2956 - acc: 0.8850 - val_loss: 0.9579 - val_acc: 0.6600\n",
      "Epoch 78/1000\n",
      "1000/1000 [==============================] - 0s 70us/step - loss: 0.2896 - acc: 0.8880 - val_loss: 1.0101 - val_acc: 0.6500\n",
      "Epoch 79/1000\n",
      "1000/1000 [==============================] - 0s 58us/step - loss: 0.2960 - acc: 0.8850 - val_loss: 0.9528 - val_acc: 0.6700\n",
      "Epoch 80/1000\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 0.2944 - acc: 0.8970 - val_loss: 0.9880 - val_acc: 0.6550\n",
      "Epoch 81/1000\n",
      "1000/1000 [==============================] - 0s 75us/step - loss: 0.2923 - acc: 0.8900 - val_loss: 0.9952 - val_acc: 0.6700\n",
      "Epoch 82/1000\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.2779 - acc: 0.8910 - val_loss: 0.9846 - val_acc: 0.6500\n",
      "Epoch 83/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.2785 - acc: 0.8940 - val_loss: 1.0260 - val_acc: 0.6500\n",
      "Epoch 84/1000\n",
      "1000/1000 [==============================] - 0s 69us/step - loss: 0.2763 - acc: 0.8940 - val_loss: 1.0179 - val_acc: 0.6600\n",
      "Epoch 85/1000\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.2832 - acc: 0.8900 - val_loss: 1.0476 - val_acc: 0.6500\n",
      "Epoch 86/1000\n",
      "1000/1000 [==============================] - 0s 67us/step - loss: 0.2786 - acc: 0.8910 - val_loss: 0.9893 - val_acc: 0.6600\n",
      "Epoch 87/1000\n",
      "1000/1000 [==============================] - 0s 68us/step - loss: 0.2644 - acc: 0.9000 - val_loss: 1.0414 - val_acc: 0.6500\n",
      "Epoch 88/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.2668 - acc: 0.8970 - val_loss: 1.0542 - val_acc: 0.6550\n",
      "Epoch 89/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.2686 - acc: 0.9000 - val_loss: 1.0550 - val_acc: 0.6450\n",
      "Epoch 90/1000\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 0.2680 - acc: 0.9000 - val_loss: 1.0613 - val_acc: 0.6450\n",
      "Epoch 91/1000\n",
      "1000/1000 [==============================] - 0s 75us/step - loss: 0.2622 - acc: 0.9030 - val_loss: 1.0508 - val_acc: 0.6450\n",
      "Epoch 92/1000\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 0.2612 - acc: 0.8930 - val_loss: 1.0585 - val_acc: 0.6500\n",
      "Epoch 93/1000\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 0.2619 - acc: 0.8990 - val_loss: 1.0586 - val_acc: 0.6450\n",
      "Epoch 94/1000\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 0.2529 - acc: 0.9040 - val_loss: 1.1168 - val_acc: 0.6300\n",
      "Epoch 95/1000\n",
      "1000/1000 [==============================] - 0s 75us/step - loss: 0.2521 - acc: 0.9000 - val_loss: 1.0778 - val_acc: 0.6550\n",
      "Epoch 96/1000\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.2538 - acc: 0.9040 - val_loss: 1.1158 - val_acc: 0.6600\n",
      "Epoch 97/1000\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.2411 - acc: 0.9150 - val_loss: 1.1020 - val_acc: 0.6450\n",
      "Epoch 98/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.2433 - acc: 0.9080 - val_loss: 1.1489 - val_acc: 0.6450\n",
      "Epoch 99/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.2387 - acc: 0.9080 - val_loss: 1.1461 - val_acc: 0.6450\n",
      "Epoch 100/1000\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 0.2468 - acc: 0.9040 - val_loss: 1.1514 - val_acc: 0.6600\n",
      "Epoch 101/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.2432 - acc: 0.9000 - val_loss: 1.1395 - val_acc: 0.6650\n",
      "Epoch 102/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.2445 - acc: 0.9080 - val_loss: 1.1810 - val_acc: 0.6450\n",
      "Epoch 103/1000\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.2307 - acc: 0.9140 - val_loss: 1.1918 - val_acc: 0.6350\n",
      "Epoch 104/1000\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.2334 - acc: 0.9110 - val_loss: 1.2523 - val_acc: 0.6400\n",
      "Epoch 105/1000\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.2304 - acc: 0.9080 - val_loss: 1.2664 - val_acc: 0.6350\n",
      "Epoch 106/1000\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.2339 - acc: 0.9070 - val_loss: 1.2639 - val_acc: 0.6400\n",
      "Epoch 107/1000\n",
      "1000/1000 [==============================] - 0s 57us/step - loss: 0.2241 - acc: 0.9140 - val_loss: 1.2359 - val_acc: 0.6300\n",
      "Epoch 108/1000\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.2239 - acc: 0.9230 - val_loss: 1.2769 - val_acc: 0.6350\n",
      "Epoch 109/1000\n",
      "1000/1000 [==============================] - 0s 67us/step - loss: 0.2274 - acc: 0.9160 - val_loss: 1.2547 - val_acc: 0.6400\n",
      "Epoch 110/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.2212 - acc: 0.9150 - val_loss: 1.2416 - val_acc: 0.6500\n",
      "Epoch 111/1000\n",
      "1000/1000 [==============================] - 0s 68us/step - loss: 0.2201 - acc: 0.9160 - val_loss: 1.2655 - val_acc: 0.6550\n",
      "Epoch 112/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.2127 - acc: 0.9260 - val_loss: 1.2607 - val_acc: 0.6550\n",
      "Epoch 113/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.2144 - acc: 0.9230 - val_loss: 1.2656 - val_acc: 0.6550\n",
      "Epoch 114/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.2151 - acc: 0.9180 - val_loss: 1.2649 - val_acc: 0.6600\n",
      "Epoch 115/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.2250 - acc: 0.9100 - val_loss: 1.3078 - val_acc: 0.6300\n",
      "Epoch 116/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.2133 - acc: 0.9210 - val_loss: 1.2949 - val_acc: 0.6450\n",
      "Epoch 117/1000\n",
      "1000/1000 [==============================] - 0s 58us/step - loss: 0.2126 - acc: 0.9220 - val_loss: 1.3589 - val_acc: 0.6350\n",
      "Epoch 118/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.2115 - acc: 0.9190 - val_loss: 1.3366 - val_acc: 0.6550\n",
      "Epoch 119/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.2085 - acc: 0.9220 - val_loss: 1.3166 - val_acc: 0.6800\n",
      "Epoch 120/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.2105 - acc: 0.9230 - val_loss: 1.3282 - val_acc: 0.6600\n",
      "Epoch 121/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.1973 - acc: 0.9260 - val_loss: 1.3375 - val_acc: 0.6600\n",
      "Epoch 122/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.1933 - acc: 0.9220 - val_loss: 1.3958 - val_acc: 0.6400\n",
      "Epoch 123/1000\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 0.1941 - acc: 0.9260 - val_loss: 1.3626 - val_acc: 0.6550\n",
      "Epoch 124/1000\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.2013 - acc: 0.9270 - val_loss: 1.4111 - val_acc: 0.6500\n",
      "Epoch 125/1000\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.1920 - acc: 0.9320 - val_loss: 1.3525 - val_acc: 0.6600\n",
      "Epoch 126/1000\n",
      "1000/1000 [==============================] - 0s 70us/step - loss: 0.1945 - acc: 0.9380 - val_loss: 1.3995 - val_acc: 0.6450\n",
      "Epoch 127/1000\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 0.1929 - acc: 0.9320 - val_loss: 1.4408 - val_acc: 0.6500\n",
      "Epoch 128/1000\n",
      "1000/1000 [==============================] - 0s 69us/step - loss: 0.1950 - acc: 0.9280 - val_loss: 1.4089 - val_acc: 0.6500\n",
      "Epoch 129/1000\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 0.1869 - acc: 0.9300 - val_loss: 1.4110 - val_acc: 0.6650\n",
      "Epoch 130/1000\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.1798 - acc: 0.9380 - val_loss: 1.4253 - val_acc: 0.6400\n",
      "Epoch 131/1000\n",
      "1000/1000 [==============================] - 0s 74us/step - loss: 0.1934 - acc: 0.9290 - val_loss: 1.4402 - val_acc: 0.6550\n",
      "Epoch 132/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.1832 - acc: 0.9320 - val_loss: 1.4832 - val_acc: 0.6600\n",
      "Epoch 133/1000\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.1845 - acc: 0.9320 - val_loss: 1.4854 - val_acc: 0.6500\n",
      "Epoch 134/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.1800 - acc: 0.9400 - val_loss: 1.4959 - val_acc: 0.6500\n",
      "Epoch 135/1000\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 0.1769 - acc: 0.9360 - val_loss: 1.4811 - val_acc: 0.6500\n",
      "Epoch 136/1000\n",
      "1000/1000 [==============================] - 0s 52us/step - loss: 0.1790 - acc: 0.9390 - val_loss: 1.4650 - val_acc: 0.6450\n",
      "Epoch 137/1000\n",
      "1000/1000 [==============================] - 0s 78us/step - loss: 0.1880 - acc: 0.9340 - val_loss: 1.4859 - val_acc: 0.6450\n",
      "Epoch 138/1000\n",
      "1000/1000 [==============================] - 0s 68us/step - loss: 0.1836 - acc: 0.9400 - val_loss: 1.4725 - val_acc: 0.6400\n",
      "Epoch 139/1000\n",
      "1000/1000 [==============================] - 0s 68us/step - loss: 0.1739 - acc: 0.9420 - val_loss: 1.5319 - val_acc: 0.6500\n",
      "Epoch 140/1000\n",
      "1000/1000 [==============================] - 0s 57us/step - loss: 0.1757 - acc: 0.9360 - val_loss: 1.5567 - val_acc: 0.6350\n",
      "Epoch 141/1000\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 0.1764 - acc: 0.9450 - val_loss: 1.5302 - val_acc: 0.6400\n",
      "Epoch 142/1000\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 0.1774 - acc: 0.9370 - val_loss: 1.5745 - val_acc: 0.6500\n",
      "Epoch 143/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.1755 - acc: 0.9410 - val_loss: 1.6335 - val_acc: 0.6350\n",
      "Epoch 144/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.1670 - acc: 0.9420 - val_loss: 1.5474 - val_acc: 0.6350\n",
      "Epoch 145/1000\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.1724 - acc: 0.9400 - val_loss: 1.5939 - val_acc: 0.6350\n",
      "Epoch 146/1000\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.1611 - acc: 0.9430 - val_loss: 1.5724 - val_acc: 0.6400\n",
      "Epoch 147/1000\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.1683 - acc: 0.9430 - val_loss: 1.5641 - val_acc: 0.6400\n",
      "Epoch 148/1000\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.1783 - acc: 0.9350 - val_loss: 1.5949 - val_acc: 0.6450\n",
      "Epoch 149/1000\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 0.1779 - acc: 0.9390 - val_loss: 1.6579 - val_acc: 0.6350\n",
      "Epoch 150/1000\n",
      "1000/1000 [==============================] - 0s 53us/step - loss: 0.1600 - acc: 0.9470 - val_loss: 1.7142 - val_acc: 0.6300\n",
      "Epoch 151/1000\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.1613 - acc: 0.9470 - val_loss: 1.6834 - val_acc: 0.6400\n",
      "Epoch 152/1000\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 0.1574 - acc: 0.9500 - val_loss: 1.6480 - val_acc: 0.6300\n",
      "Epoch 153/1000\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.1638 - acc: 0.9400 - val_loss: 1.7113 - val_acc: 0.6350\n",
      "Epoch 154/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.1604 - acc: 0.9460 - val_loss: 1.6769 - val_acc: 0.6450\n",
      "Epoch 155/1000\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.1618 - acc: 0.9470 - val_loss: 1.6875 - val_acc: 0.6350\n",
      "Epoch 156/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.1600 - acc: 0.9440 - val_loss: 1.6914 - val_acc: 0.6350\n",
      "Epoch 157/1000\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 0.1560 - acc: 0.9460 - val_loss: 1.7534 - val_acc: 0.6300\n",
      "Epoch 158/1000\n",
      "1000/1000 [==============================] - 0s 71us/step - loss: 0.1579 - acc: 0.9440 - val_loss: 1.7260 - val_acc: 0.6500\n",
      "Epoch 159/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.1643 - acc: 0.9380 - val_loss: 1.6178 - val_acc: 0.6500\n",
      "Epoch 160/1000\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.1535 - acc: 0.9510 - val_loss: 1.7971 - val_acc: 0.6450\n",
      "Epoch 161/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.1513 - acc: 0.9520 - val_loss: 1.7955 - val_acc: 0.6350\n",
      "Epoch 162/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.1521 - acc: 0.9480 - val_loss: 1.8248 - val_acc: 0.6350\n",
      "Epoch 163/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.1530 - acc: 0.9440 - val_loss: 1.7785 - val_acc: 0.6450\n",
      "Epoch 164/1000\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 0.1429 - acc: 0.9560 - val_loss: 1.8433 - val_acc: 0.6300\n",
      "Epoch 165/1000\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 0.1391 - acc: 0.9540 - val_loss: 1.7160 - val_acc: 0.6350\n",
      "Epoch 166/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.1496 - acc: 0.9510 - val_loss: 1.7920 - val_acc: 0.6350\n",
      "Epoch 167/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.1398 - acc: 0.9500 - val_loss: 1.8188 - val_acc: 0.6400\n",
      "Epoch 168/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.1483 - acc: 0.9510 - val_loss: 1.9016 - val_acc: 0.6400\n",
      "Epoch 169/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.1468 - acc: 0.9540 - val_loss: 1.7247 - val_acc: 0.6450\n",
      "Epoch 170/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.1458 - acc: 0.9450 - val_loss: 1.8793 - val_acc: 0.6350\n",
      "Epoch 171/1000\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 0.1341 - acc: 0.9510 - val_loss: 1.8773 - val_acc: 0.6350\n",
      "Epoch 172/1000\n",
      "1000/1000 [==============================] - 0s 58us/step - loss: 0.1425 - acc: 0.9490 - val_loss: 1.8365 - val_acc: 0.6550\n",
      "Epoch 173/1000\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 0.1410 - acc: 0.9500 - val_loss: 1.8414 - val_acc: 0.6600\n",
      "Epoch 174/1000\n",
      "1000/1000 [==============================] - 0s 75us/step - loss: 0.1357 - acc: 0.9530 - val_loss: 1.8022 - val_acc: 0.6450\n",
      "Epoch 175/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.1292 - acc: 0.9620 - val_loss: 1.8775 - val_acc: 0.6400\n",
      "Epoch 176/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.1283 - acc: 0.9590 - val_loss: 1.8565 - val_acc: 0.6450\n",
      "Epoch 177/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.1341 - acc: 0.9580 - val_loss: 1.8491 - val_acc: 0.6500\n",
      "Epoch 178/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.1328 - acc: 0.9560 - val_loss: 1.9479 - val_acc: 0.6550\n",
      "Epoch 179/1000\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.1308 - acc: 0.9570 - val_loss: 1.9510 - val_acc: 0.6450\n",
      "Epoch 180/1000\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 0.1275 - acc: 0.9590 - val_loss: 1.8938 - val_acc: 0.6500\n",
      "Epoch 181/1000\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.1310 - acc: 0.9600 - val_loss: 1.9240 - val_acc: 0.6550\n",
      "Epoch 182/1000\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.1278 - acc: 0.9560 - val_loss: 1.9538 - val_acc: 0.6350\n",
      "Epoch 183/1000\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 0.1278 - acc: 0.9580 - val_loss: 2.0033 - val_acc: 0.6400\n",
      "Epoch 184/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.1271 - acc: 0.9590 - val_loss: 2.0153 - val_acc: 0.6400\n",
      "Epoch 185/1000\n",
      "1000/1000 [==============================] - 0s 55us/step - loss: 0.1328 - acc: 0.9560 - val_loss: 1.9898 - val_acc: 0.6450\n",
      "Epoch 186/1000\n",
      "1000/1000 [==============================] - 0s 73us/step - loss: 0.1241 - acc: 0.9620 - val_loss: 1.9754 - val_acc: 0.6550\n",
      "Epoch 187/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.1264 - acc: 0.9580 - val_loss: 1.8856 - val_acc: 0.6600\n",
      "Epoch 188/1000\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.1227 - acc: 0.9630 - val_loss: 2.0406 - val_acc: 0.6550\n",
      "Epoch 189/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.1185 - acc: 0.9670 - val_loss: 1.9577 - val_acc: 0.6500\n",
      "Epoch 190/1000\n",
      "1000/1000 [==============================] - 0s 55us/step - loss: 0.1273 - acc: 0.9560 - val_loss: 2.0714 - val_acc: 0.6450\n",
      "Epoch 191/1000\n",
      "1000/1000 [==============================] - 0s 67us/step - loss: 0.1208 - acc: 0.9590 - val_loss: 2.0618 - val_acc: 0.6450\n",
      "Epoch 192/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.1261 - acc: 0.9560 - val_loss: 2.0045 - val_acc: 0.6600\n",
      "Epoch 193/1000\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 0.1219 - acc: 0.9610 - val_loss: 2.0794 - val_acc: 0.6400\n",
      "Epoch 194/1000\n",
      "1000/1000 [==============================] - 0s 68us/step - loss: 0.1230 - acc: 0.9600 - val_loss: 2.0077 - val_acc: 0.6650\n",
      "Epoch 195/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.1176 - acc: 0.9630 - val_loss: 2.0569 - val_acc: 0.6550\n",
      "Epoch 196/1000\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.1153 - acc: 0.9640 - val_loss: 2.1086 - val_acc: 0.6550\n",
      "Epoch 197/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.1159 - acc: 0.9600 - val_loss: 2.1282 - val_acc: 0.6500\n",
      "Epoch 198/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.1182 - acc: 0.9640 - val_loss: 2.0825 - val_acc: 0.6550\n",
      "Epoch 199/1000\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.1058 - acc: 0.9700 - val_loss: 2.1852 - val_acc: 0.6450\n",
      "Epoch 200/1000\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.1249 - acc: 0.9570 - val_loss: 2.0776 - val_acc: 0.6450\n",
      "Epoch 201/1000\n",
      "1000/1000 [==============================] - 0s 58us/step - loss: 0.1201 - acc: 0.9610 - val_loss: 2.0681 - val_acc: 0.6500\n",
      "Epoch 202/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.1122 - acc: 0.9580 - val_loss: 2.1381 - val_acc: 0.6350\n",
      "Epoch 203/1000\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 0.1135 - acc: 0.9660 - val_loss: 2.0768 - val_acc: 0.6600\n",
      "Epoch 204/1000\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 0.1058 - acc: 0.9690 - val_loss: 2.0960 - val_acc: 0.6450\n",
      "Epoch 205/1000\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 0.1121 - acc: 0.9630 - val_loss: 2.0846 - val_acc: 0.6700\n",
      "Epoch 206/1000\n",
      "1000/1000 [==============================] - 0s 56us/step - loss: 0.1086 - acc: 0.9640 - val_loss: 2.1837 - val_acc: 0.6400\n",
      "Epoch 207/1000\n",
      "1000/1000 [==============================] - 0s 67us/step - loss: 0.1113 - acc: 0.9640 - val_loss: 2.1028 - val_acc: 0.6550\n",
      "Epoch 208/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.1065 - acc: 0.9680 - val_loss: 2.1653 - val_acc: 0.6450\n",
      "Epoch 209/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.1196 - acc: 0.9590 - val_loss: 2.0946 - val_acc: 0.6700\n",
      "Epoch 210/1000\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.1146 - acc: 0.9660 - val_loss: 2.0743 - val_acc: 0.6550\n",
      "Epoch 211/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.1190 - acc: 0.9610 - val_loss: 2.1111 - val_acc: 0.6500\n",
      "Epoch 212/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.1052 - acc: 0.9670 - val_loss: 2.1960 - val_acc: 0.6350\n",
      "Epoch 213/1000\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 0.1104 - acc: 0.9630 - val_loss: 2.0271 - val_acc: 0.6500\n",
      "Epoch 214/1000\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 0.1056 - acc: 0.9610 - val_loss: 2.1897 - val_acc: 0.6450\n",
      "Epoch 215/1000\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 0.0972 - acc: 0.9700 - val_loss: 2.1755 - val_acc: 0.6600\n",
      "Epoch 216/1000\n",
      "1000/1000 [==============================] - 0s 51us/step - loss: 0.1015 - acc: 0.9700 - val_loss: 2.1755 - val_acc: 0.6500\n",
      "Epoch 217/1000\n",
      "1000/1000 [==============================] - 0s 68us/step - loss: 0.0980 - acc: 0.9720 - val_loss: 2.2116 - val_acc: 0.6500\n",
      "Epoch 218/1000\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 0.0972 - acc: 0.9720 - val_loss: 2.1673 - val_acc: 0.6600\n",
      "Epoch 219/1000\n",
      "1000/1000 [==============================] - 0s 72us/step - loss: 0.1014 - acc: 0.9650 - val_loss: 2.2428 - val_acc: 0.6550\n",
      "Epoch 220/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.1005 - acc: 0.9720 - val_loss: 2.2151 - val_acc: 0.6650\n",
      "Epoch 221/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.0915 - acc: 0.9760 - val_loss: 2.2314 - val_acc: 0.6650\n",
      "Epoch 222/1000\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 0.0940 - acc: 0.9730 - val_loss: 2.2114 - val_acc: 0.6600\n",
      "Epoch 223/1000\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.0998 - acc: 0.9730 - val_loss: 2.1491 - val_acc: 0.6500\n",
      "Epoch 224/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0938 - acc: 0.9730 - val_loss: 2.2539 - val_acc: 0.6600\n",
      "Epoch 225/1000\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 0.0914 - acc: 0.9720 - val_loss: 2.2033 - val_acc: 0.6500\n",
      "Epoch 226/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0913 - acc: 0.9730 - val_loss: 2.3329 - val_acc: 0.6600\n",
      "Epoch 227/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0941 - acc: 0.9720 - val_loss: 2.1290 - val_acc: 0.6500\n",
      "Epoch 228/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.1056 - acc: 0.9670 - val_loss: 2.3322 - val_acc: 0.6500\n",
      "Epoch 229/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.1008 - acc: 0.9640 - val_loss: 2.1878 - val_acc: 0.6600\n",
      "Epoch 230/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0960 - acc: 0.9700 - val_loss: 2.2670 - val_acc: 0.6500\n",
      "Epoch 231/1000\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 0.0927 - acc: 0.9730 - val_loss: 2.2688 - val_acc: 0.6650\n",
      "Epoch 232/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0846 - acc: 0.9820 - val_loss: 2.3190 - val_acc: 0.6500\n",
      "Epoch 233/1000\n",
      "1000/1000 [==============================] - 0s 58us/step - loss: 0.0827 - acc: 0.9750 - val_loss: 2.2870 - val_acc: 0.6650\n",
      "Epoch 234/1000\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 0.0828 - acc: 0.9750 - val_loss: 2.3789 - val_acc: 0.6650\n",
      "Epoch 235/1000\n",
      "1000/1000 [==============================] - 0s 73us/step - loss: 0.0851 - acc: 0.9760 - val_loss: 2.3030 - val_acc: 0.6600\n",
      "Epoch 236/1000\n",
      "1000/1000 [==============================] - 0s 57us/step - loss: 0.0843 - acc: 0.9740 - val_loss: 2.2951 - val_acc: 0.6650\n",
      "Epoch 237/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0896 - acc: 0.9750 - val_loss: 2.3539 - val_acc: 0.6500\n",
      "Epoch 238/1000\n",
      "1000/1000 [==============================] - 0s 58us/step - loss: 0.0940 - acc: 0.9700 - val_loss: 2.3003 - val_acc: 0.6450\n",
      "Epoch 239/1000\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 0.0782 - acc: 0.9780 - val_loss: 2.3924 - val_acc: 0.6550\n",
      "Epoch 240/1000\n",
      "1000/1000 [==============================] - 0s 74us/step - loss: 0.0856 - acc: 0.9790 - val_loss: 2.3321 - val_acc: 0.6450\n",
      "Epoch 241/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0855 - acc: 0.9740 - val_loss: 2.2976 - val_acc: 0.6500\n",
      "Epoch 242/1000\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.0814 - acc: 0.9790 - val_loss: 2.4390 - val_acc: 0.6650\n",
      "Epoch 243/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0857 - acc: 0.9770 - val_loss: 2.4281 - val_acc: 0.6650\n",
      "Epoch 244/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0779 - acc: 0.9790 - val_loss: 2.3906 - val_acc: 0.6500\n",
      "Epoch 245/1000\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.0765 - acc: 0.9800 - val_loss: 2.3787 - val_acc: 0.6550\n",
      "Epoch 246/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.0837 - acc: 0.9750 - val_loss: 2.4384 - val_acc: 0.6500\n",
      "Epoch 247/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0779 - acc: 0.9780 - val_loss: 2.3628 - val_acc: 0.6600\n",
      "Epoch 248/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0992 - acc: 0.9680 - val_loss: 2.1987 - val_acc: 0.6650\n",
      "Epoch 249/1000\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 0.1056 - acc: 0.9670 - val_loss: 2.2604 - val_acc: 0.6550\n",
      "Epoch 250/1000\n",
      "1000/1000 [==============================] - 0s 69us/step - loss: 0.1077 - acc: 0.9650 - val_loss: 2.3380 - val_acc: 0.6450\n",
      "Epoch 251/1000\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 0.0883 - acc: 0.9750 - val_loss: 2.2570 - val_acc: 0.6650\n",
      "Epoch 252/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.0895 - acc: 0.9770 - val_loss: 2.5043 - val_acc: 0.6450\n",
      "Epoch 253/1000\n",
      "1000/1000 [==============================] - 0s 53us/step - loss: 0.1027 - acc: 0.9700 - val_loss: 2.4009 - val_acc: 0.6550\n",
      "Epoch 254/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.0887 - acc: 0.9750 - val_loss: 2.5007 - val_acc: 0.6500\n",
      "Epoch 255/1000\n",
      "1000/1000 [==============================] - 0s 68us/step - loss: 0.0844 - acc: 0.9710 - val_loss: 2.4526 - val_acc: 0.6550\n",
      "Epoch 256/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0811 - acc: 0.9760 - val_loss: 2.4239 - val_acc: 0.6650\n",
      "Epoch 257/1000\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 0.0767 - acc: 0.9790 - val_loss: 2.5545 - val_acc: 0.6650\n",
      "Epoch 258/1000\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 0.0808 - acc: 0.9770 - val_loss: 2.4649 - val_acc: 0.6400\n",
      "Epoch 259/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0811 - acc: 0.9770 - val_loss: 2.4704 - val_acc: 0.6600\n",
      "Epoch 260/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0759 - acc: 0.9820 - val_loss: 2.4657 - val_acc: 0.6550\n",
      "Epoch 261/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0748 - acc: 0.9790 - val_loss: 2.4957 - val_acc: 0.6650\n",
      "Epoch 262/1000\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.0750 - acc: 0.9790 - val_loss: 2.4923 - val_acc: 0.6650\n",
      "Epoch 263/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0775 - acc: 0.9780 - val_loss: 2.5469 - val_acc: 0.6650\n",
      "Epoch 264/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.0641 - acc: 0.9850 - val_loss: 2.5314 - val_acc: 0.6600\n",
      "Epoch 265/1000\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 0.0649 - acc: 0.9830 - val_loss: 2.5242 - val_acc: 0.6550\n",
      "Epoch 266/1000\n",
      "1000/1000 [==============================] - 0s 57us/step - loss: 0.0728 - acc: 0.9770 - val_loss: 2.4188 - val_acc: 0.6700\n",
      "Epoch 267/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0792 - acc: 0.9770 - val_loss: 2.5415 - val_acc: 0.6700\n",
      "Epoch 268/1000\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 0.0780 - acc: 0.9790 - val_loss: 2.4260 - val_acc: 0.6550\n",
      "Epoch 269/1000\n",
      "1000/1000 [==============================] - 0s 73us/step - loss: 0.0769 - acc: 0.9780 - val_loss: 2.4842 - val_acc: 0.6500\n",
      "Epoch 270/1000\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 0.0762 - acc: 0.9770 - val_loss: 2.6284 - val_acc: 0.6400\n",
      "Epoch 271/1000\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 0.0758 - acc: 0.9770 - val_loss: 2.5506 - val_acc: 0.6600\n",
      "Epoch 272/1000\n",
      "1000/1000 [==============================] - 0s 72us/step - loss: 0.0757 - acc: 0.9790 - val_loss: 2.4459 - val_acc: 0.6600\n",
      "Epoch 273/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0718 - acc: 0.9790 - val_loss: 2.5132 - val_acc: 0.6650\n",
      "Epoch 274/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0834 - acc: 0.9750 - val_loss: 2.5153 - val_acc: 0.6800\n",
      "Epoch 275/1000\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.0736 - acc: 0.9810 - val_loss: 2.5050 - val_acc: 0.6500\n",
      "Epoch 276/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0719 - acc: 0.9780 - val_loss: 2.4864 - val_acc: 0.6600\n",
      "Epoch 277/1000\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.0672 - acc: 0.9830 - val_loss: 2.5286 - val_acc: 0.6600\n",
      "Epoch 278/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0697 - acc: 0.9790 - val_loss: 2.5883 - val_acc: 0.6800\n",
      "Epoch 279/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0672 - acc: 0.9830 - val_loss: 2.5766 - val_acc: 0.6600\n",
      "Epoch 280/1000\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.0663 - acc: 0.9820 - val_loss: 2.5173 - val_acc: 0.6600\n",
      "Epoch 281/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0678 - acc: 0.9830 - val_loss: 2.5397 - val_acc: 0.6650\n",
      "Epoch 282/1000\n",
      "1000/1000 [==============================] - 0s 52us/step - loss: 0.0678 - acc: 0.9800 - val_loss: 2.5844 - val_acc: 0.6650\n",
      "Epoch 283/1000\n",
      "1000/1000 [==============================] - 0s 71us/step - loss: 0.0684 - acc: 0.9820 - val_loss: 2.5578 - val_acc: 0.6650\n",
      "Epoch 284/1000\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 0.0708 - acc: 0.9810 - val_loss: 2.5376 - val_acc: 0.6700\n",
      "Epoch 285/1000\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 0.0665 - acc: 0.9810 - val_loss: 2.6016 - val_acc: 0.6600\n",
      "Epoch 286/1000\n",
      "1000/1000 [==============================] - 0s 71us/step - loss: 0.0636 - acc: 0.9840 - val_loss: 2.5630 - val_acc: 0.6550\n",
      "Epoch 287/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0624 - acc: 0.9840 - val_loss: 2.6327 - val_acc: 0.6600\n",
      "Epoch 288/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0638 - acc: 0.9840 - val_loss: 2.5022 - val_acc: 0.6500\n",
      "Epoch 289/1000\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.0643 - acc: 0.9860 - val_loss: 2.5455 - val_acc: 0.6800\n",
      "Epoch 290/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0642 - acc: 0.9840 - val_loss: 2.6043 - val_acc: 0.6750\n",
      "Epoch 291/1000\n",
      "1000/1000 [==============================] - 0s 72us/step - loss: 0.0561 - acc: 0.9850 - val_loss: 2.5622 - val_acc: 0.6750\n",
      "Epoch 292/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0602 - acc: 0.9840 - val_loss: 2.6123 - val_acc: 0.6750\n",
      "Epoch 293/1000\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 0.0616 - acc: 0.9850 - val_loss: 2.5363 - val_acc: 0.6850\n",
      "Epoch 294/1000\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.0622 - acc: 0.9840 - val_loss: 2.6099 - val_acc: 0.6750\n",
      "Epoch 295/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0637 - acc: 0.9810 - val_loss: 2.5713 - val_acc: 0.6750\n",
      "Epoch 296/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0614 - acc: 0.9830 - val_loss: 2.6550 - val_acc: 0.6600\n",
      "Epoch 297/1000\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.0725 - acc: 0.9820 - val_loss: 2.6651 - val_acc: 0.6650\n",
      "Epoch 298/1000\n",
      "1000/1000 [==============================] - 0s 67us/step - loss: 0.0832 - acc: 0.9770 - val_loss: 2.5508 - val_acc: 0.6750\n",
      "Epoch 299/1000\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 0.0847 - acc: 0.9740 - val_loss: 2.6080 - val_acc: 0.6650\n",
      "Epoch 300/1000\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 0.1071 - acc: 0.9670 - val_loss: 2.5329 - val_acc: 0.6600\n",
      "Epoch 301/1000\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 0.1491 - acc: 0.9550 - val_loss: 2.5321 - val_acc: 0.6450\n",
      "Epoch 302/1000\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 0.0976 - acc: 0.9670 - val_loss: 2.5921 - val_acc: 0.6550\n",
      "Epoch 303/1000\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 0.0752 - acc: 0.9770 - val_loss: 2.6364 - val_acc: 0.6700\n",
      "Epoch 304/1000\n",
      "1000/1000 [==============================] - 0s 73us/step - loss: 0.0806 - acc: 0.9770 - val_loss: 2.5213 - val_acc: 0.6700\n",
      "Epoch 305/1000\n",
      "1000/1000 [==============================] - 0s 67us/step - loss: 0.0692 - acc: 0.9810 - val_loss: 2.5056 - val_acc: 0.6700\n",
      "Epoch 306/1000\n",
      "1000/1000 [==============================] - 0s 67us/step - loss: 0.0602 - acc: 0.9850 - val_loss: 2.5725 - val_acc: 0.6700\n",
      "Epoch 307/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0623 - acc: 0.9840 - val_loss: 2.5422 - val_acc: 0.6650\n",
      "Epoch 308/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0649 - acc: 0.9820 - val_loss: 2.5356 - val_acc: 0.6600\n",
      "Epoch 309/1000\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 0.0620 - acc: 0.9850 - val_loss: 2.5799 - val_acc: 0.6700\n",
      "Epoch 310/1000\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 0.0624 - acc: 0.9830 - val_loss: 2.5535 - val_acc: 0.6650\n",
      "Epoch 311/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0663 - acc: 0.9820 - val_loss: 2.6270 - val_acc: 0.6700\n",
      "Epoch 312/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0620 - acc: 0.9830 - val_loss: 2.6098 - val_acc: 0.6850\n",
      "Epoch 313/1000\n",
      "1000/1000 [==============================] - 0s 58us/step - loss: 0.0647 - acc: 0.9830 - val_loss: 2.6373 - val_acc: 0.6650\n",
      "Epoch 314/1000\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.0597 - acc: 0.9840 - val_loss: 2.6031 - val_acc: 0.6800\n",
      "Epoch 315/1000\n",
      "1000/1000 [==============================] - 0s 56us/step - loss: 0.0628 - acc: 0.9830 - val_loss: 2.5740 - val_acc: 0.6650\n",
      "Epoch 316/1000\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 0.0577 - acc: 0.9840 - val_loss: 2.6014 - val_acc: 0.6750\n",
      "Epoch 317/1000\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 0.0629 - acc: 0.9840 - val_loss: 2.6906 - val_acc: 0.6450\n",
      "Epoch 318/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.0642 - acc: 0.9840 - val_loss: 2.6083 - val_acc: 0.6700\n",
      "Epoch 319/1000\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.0592 - acc: 0.9820 - val_loss: 2.7334 - val_acc: 0.6550\n",
      "Epoch 320/1000\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 0.0683 - acc: 0.9810 - val_loss: 2.6862 - val_acc: 0.6650\n",
      "Epoch 321/1000\n",
      "1000/1000 [==============================] - 0s 71us/step - loss: 0.0557 - acc: 0.9840 - val_loss: 2.7217 - val_acc: 0.6650\n",
      "Epoch 322/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0588 - acc: 0.9840 - val_loss: 2.7625 - val_acc: 0.6650\n",
      "Epoch 323/1000\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.0600 - acc: 0.9830 - val_loss: 2.6786 - val_acc: 0.6600\n",
      "Epoch 324/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0585 - acc: 0.9860 - val_loss: 2.7713 - val_acc: 0.6550\n",
      "Epoch 325/1000\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 0.0544 - acc: 0.9870 - val_loss: 2.7118 - val_acc: 0.6600\n",
      "Epoch 326/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0529 - acc: 0.9870 - val_loss: 2.8062 - val_acc: 0.6550\n",
      "Epoch 327/1000\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.0559 - acc: 0.9840 - val_loss: 2.7270 - val_acc: 0.6600\n",
      "Epoch 328/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0499 - acc: 0.9860 - val_loss: 2.7290 - val_acc: 0.6650\n",
      "Epoch 329/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0560 - acc: 0.9860 - val_loss: 2.7898 - val_acc: 0.6650\n",
      "Epoch 330/1000\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.0574 - acc: 0.9860 - val_loss: 2.7976 - val_acc: 0.6650\n",
      "Epoch 331/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.0549 - acc: 0.9860 - val_loss: 2.7604 - val_acc: 0.6450\n",
      "Epoch 332/1000\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 0.0536 - acc: 0.9880 - val_loss: 2.7682 - val_acc: 0.6650\n",
      "Epoch 333/1000\n",
      "1000/1000 [==============================] - 0s 71us/step - loss: 0.0591 - acc: 0.9820 - val_loss: 2.7272 - val_acc: 0.6600\n",
      "Epoch 334/1000\n",
      "1000/1000 [==============================] - 0s 57us/step - loss: 0.0591 - acc: 0.9850 - val_loss: 2.7966 - val_acc: 0.6650\n",
      "Epoch 335/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0587 - acc: 0.9830 - val_loss: 2.8610 - val_acc: 0.6600\n",
      "Epoch 336/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0542 - acc: 0.9870 - val_loss: 2.7668 - val_acc: 0.6700\n",
      "Epoch 337/1000\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 0.0659 - acc: 0.9820 - val_loss: 2.8547 - val_acc: 0.6650\n",
      "Epoch 338/1000\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.0645 - acc: 0.9810 - val_loss: 2.6717 - val_acc: 0.6800\n",
      "Epoch 339/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.1142 - acc: 0.9770 - val_loss: 2.6445 - val_acc: 0.6750\n",
      "Epoch 340/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.1118 - acc: 0.9630 - val_loss: 2.7756 - val_acc: 0.6500\n",
      "Epoch 341/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.1031 - acc: 0.9620 - val_loss: 2.8243 - val_acc: 0.6500\n",
      "Epoch 342/1000\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 0.0665 - acc: 0.9820 - val_loss: 2.6659 - val_acc: 0.6450\n",
      "Epoch 343/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0592 - acc: 0.9850 - val_loss: 2.7097 - val_acc: 0.6450\n",
      "Epoch 344/1000\n",
      "1000/1000 [==============================] - 0s 124us/step - loss: 0.0502 - acc: 0.9870 - val_loss: 2.7554 - val_acc: 0.6500\n",
      "Epoch 345/1000\n",
      "1000/1000 [==============================] - 0s 70us/step - loss: 0.0588 - acc: 0.9850 - val_loss: 2.7233 - val_acc: 0.6450\n",
      "Epoch 346/1000\n",
      "1000/1000 [==============================] - 0s 70us/step - loss: 0.0491 - acc: 0.9880 - val_loss: 2.8248 - val_acc: 0.6500\n",
      "Epoch 347/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.0541 - acc: 0.9860 - val_loss: 2.8302 - val_acc: 0.6600\n",
      "Epoch 348/1000\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.0521 - acc: 0.9860 - val_loss: 2.8252 - val_acc: 0.6550\n",
      "Epoch 349/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.0527 - acc: 0.9860 - val_loss: 2.7923 - val_acc: 0.6600\n",
      "Epoch 350/1000\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 0.0540 - acc: 0.9850 - val_loss: 2.7571 - val_acc: 0.6650\n",
      "Epoch 351/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.0549 - acc: 0.9860 - val_loss: 2.8584 - val_acc: 0.6550\n",
      "Epoch 352/1000\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.0487 - acc: 0.9880 - val_loss: 2.8667 - val_acc: 0.6550\n",
      "Epoch 353/1000\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.0479 - acc: 0.9880 - val_loss: 2.8881 - val_acc: 0.6650\n",
      "Epoch 354/1000\n",
      "1000/1000 [==============================] - 0s 78us/step - loss: 0.0586 - acc: 0.9850 - val_loss: 2.8866 - val_acc: 0.6500\n",
      "Epoch 355/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 66us/step - loss: 0.0522 - acc: 0.9860 - val_loss: 2.8359 - val_acc: 0.6650\n",
      "Epoch 356/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0471 - acc: 0.9890 - val_loss: 2.8967 - val_acc: 0.6700\n",
      "Epoch 357/1000\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.0498 - acc: 0.9860 - val_loss: 2.9085 - val_acc: 0.6750\n",
      "Epoch 358/1000\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 0.0505 - acc: 0.9870 - val_loss: 2.9253 - val_acc: 0.6700\n",
      "Epoch 359/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0676 - acc: 0.9800 - val_loss: 2.8781 - val_acc: 0.6450\n",
      "Epoch 360/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0876 - acc: 0.9750 - val_loss: 2.8016 - val_acc: 0.6500\n",
      "Epoch 361/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0800 - acc: 0.9750 - val_loss: 2.8572 - val_acc: 0.6500\n",
      "Epoch 362/1000\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 0.0763 - acc: 0.9740 - val_loss: 2.6697 - val_acc: 0.6500\n",
      "Epoch 363/1000\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 0.0632 - acc: 0.9820 - val_loss: 2.9110 - val_acc: 0.6500\n",
      "Epoch 364/1000\n",
      "1000/1000 [==============================] - 0s 68us/step - loss: 0.0707 - acc: 0.9790 - val_loss: 2.8034 - val_acc: 0.6550\n",
      "Epoch 365/1000\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.0638 - acc: 0.9820 - val_loss: 2.8344 - val_acc: 0.6650\n",
      "Epoch 366/1000\n",
      "1000/1000 [==============================] - 0s 58us/step - loss: 0.0482 - acc: 0.9880 - val_loss: 2.9046 - val_acc: 0.6550\n",
      "Epoch 367/1000\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 0.0576 - acc: 0.9860 - val_loss: 2.7943 - val_acc: 0.6650\n",
      "Epoch 368/1000\n",
      "1000/1000 [==============================] - 0s 74us/step - loss: 0.0571 - acc: 0.9830 - val_loss: 2.8320 - val_acc: 0.6400\n",
      "Epoch 369/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0524 - acc: 0.9850 - val_loss: 2.8300 - val_acc: 0.6500\n",
      "Epoch 370/1000\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.0525 - acc: 0.9860 - val_loss: 2.9023 - val_acc: 0.6400\n",
      "Epoch 371/1000\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.0526 - acc: 0.9840 - val_loss: 2.8401 - val_acc: 0.6550\n",
      "Epoch 372/1000\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 0.0514 - acc: 0.9870 - val_loss: 2.9252 - val_acc: 0.6400\n",
      "Epoch 373/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0481 - acc: 0.9870 - val_loss: 2.9207 - val_acc: 0.6400\n",
      "Epoch 374/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0448 - acc: 0.9890 - val_loss: 2.9203 - val_acc: 0.6350\n",
      "Epoch 375/1000\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 0.0454 - acc: 0.9870 - val_loss: 2.9384 - val_acc: 0.6350\n",
      "Epoch 376/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0462 - acc: 0.9900 - val_loss: 2.9749 - val_acc: 0.6500\n",
      "Epoch 377/1000\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 0.0452 - acc: 0.9880 - val_loss: 3.0055 - val_acc: 0.6450\n",
      "Epoch 378/1000\n",
      "1000/1000 [==============================] - 0s 68us/step - loss: 0.0520 - acc: 0.9870 - val_loss: 2.9659 - val_acc: 0.6500\n",
      "Epoch 379/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0690 - acc: 0.9790 - val_loss: 2.8944 - val_acc: 0.6550\n",
      "Epoch 380/1000\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 0.0589 - acc: 0.9800 - val_loss: 3.2127 - val_acc: 0.6450\n",
      "Epoch 381/1000\n",
      "1000/1000 [==============================] - 0s 58us/step - loss: 0.0904 - acc: 0.9750 - val_loss: 2.7209 - val_acc: 0.6600\n",
      "Epoch 382/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0724 - acc: 0.9800 - val_loss: 2.7658 - val_acc: 0.6550\n",
      "Epoch 383/1000\n",
      "1000/1000 [==============================] - 0s 58us/step - loss: 0.0746 - acc: 0.9810 - val_loss: 2.7831 - val_acc: 0.6500\n",
      "Epoch 384/1000\n",
      "1000/1000 [==============================] - 0s 56us/step - loss: 0.0666 - acc: 0.9810 - val_loss: 2.7856 - val_acc: 0.6700\n",
      "Epoch 385/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.0588 - acc: 0.9860 - val_loss: 2.7926 - val_acc: 0.6650\n",
      "Epoch 386/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0512 - acc: 0.9870 - val_loss: 2.8304 - val_acc: 0.6650\n",
      "Epoch 387/1000\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.0493 - acc: 0.9870 - val_loss: 2.8598 - val_acc: 0.6700\n",
      "Epoch 388/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0511 - acc: 0.9890 - val_loss: 2.8590 - val_acc: 0.6650\n",
      "Epoch 389/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0431 - acc: 0.9900 - val_loss: 2.9211 - val_acc: 0.6600\n",
      "Epoch 390/1000\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 0.0497 - acc: 0.9890 - val_loss: 2.9538 - val_acc: 0.6600\n",
      "Epoch 391/1000\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 0.0478 - acc: 0.9870 - val_loss: 2.8712 - val_acc: 0.6650\n",
      "Epoch 392/1000\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 0.0492 - acc: 0.9880 - val_loss: 2.9572 - val_acc: 0.6600\n",
      "Epoch 393/1000\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 0.0472 - acc: 0.9870 - val_loss: 2.9867 - val_acc: 0.6550\n",
      "Epoch 394/1000\n",
      "1000/1000 [==============================] - 0s 57us/step - loss: 0.0437 - acc: 0.9890 - val_loss: 2.9901 - val_acc: 0.6600\n",
      "Epoch 395/1000\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 0.0452 - acc: 0.9890 - val_loss: 2.9883 - val_acc: 0.6550\n",
      "Epoch 396/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0452 - acc: 0.9890 - val_loss: 3.0192 - val_acc: 0.6550\n",
      "Epoch 397/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0497 - acc: 0.9870 - val_loss: 3.0479 - val_acc: 0.6550\n",
      "Epoch 398/1000\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 0.0430 - acc: 0.9890 - val_loss: 3.0194 - val_acc: 0.6550\n",
      "Epoch 399/1000\n",
      "1000/1000 [==============================] - 0s 69us/step - loss: 0.0437 - acc: 0.9890 - val_loss: 3.0171 - val_acc: 0.6600\n",
      "Epoch 400/1000\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 0.0471 - acc: 0.9870 - val_loss: 2.9711 - val_acc: 0.6700\n",
      "Epoch 401/1000\n",
      "1000/1000 [==============================] - 0s 74us/step - loss: 0.0418 - acc: 0.9890 - val_loss: 2.9988 - val_acc: 0.6650\n",
      "Epoch 402/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0448 - acc: 0.9880 - val_loss: 2.9373 - val_acc: 0.6550\n",
      "Epoch 403/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0439 - acc: 0.9890 - val_loss: 2.8792 - val_acc: 0.6600\n",
      "Epoch 404/1000\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 0.0440 - acc: 0.9890 - val_loss: 2.9417 - val_acc: 0.6500\n",
      "Epoch 405/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0515 - acc: 0.9840 - val_loss: 2.9118 - val_acc: 0.6550\n",
      "Epoch 406/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0572 - acc: 0.9830 - val_loss: 2.8194 - val_acc: 0.6650\n",
      "Epoch 407/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0588 - acc: 0.9830 - val_loss: 2.8946 - val_acc: 0.6550\n",
      "Epoch 408/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0684 - acc: 0.9800 - val_loss: 2.8336 - val_acc: 0.6450\n",
      "Epoch 409/1000\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.0586 - acc: 0.9810 - val_loss: 3.0354 - val_acc: 0.6600\n",
      "Epoch 410/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0659 - acc: 0.9800 - val_loss: 2.9691 - val_acc: 0.6800\n",
      "Epoch 411/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0738 - acc: 0.9760 - val_loss: 2.9257 - val_acc: 0.6600\n",
      "Epoch 412/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.0964 - acc: 0.9760 - val_loss: 2.6318 - val_acc: 0.6550\n",
      "Epoch 413/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.0854 - acc: 0.9700 - val_loss: 2.7377 - val_acc: 0.6700\n",
      "Epoch 414/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.0653 - acc: 0.9810 - val_loss: 2.8367 - val_acc: 0.6600\n",
      "Epoch 415/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0693 - acc: 0.9820 - val_loss: 2.9557 - val_acc: 0.6600\n",
      "Epoch 416/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.0702 - acc: 0.9820 - val_loss: 2.7620 - val_acc: 0.6600\n",
      "Epoch 417/1000\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.0702 - acc: 0.9790 - val_loss: 2.8592 - val_acc: 0.6550\n",
      "Epoch 418/1000\n",
      "1000/1000 [==============================] - 0s 58us/step - loss: 0.0487 - acc: 0.9870 - val_loss: 2.9036 - val_acc: 0.6450\n",
      "Epoch 419/1000\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.0506 - acc: 0.9850 - val_loss: 2.9540 - val_acc: 0.6550\n",
      "Epoch 420/1000\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.0575 - acc: 0.9820 - val_loss: 2.8437 - val_acc: 0.6550\n",
      "Epoch 421/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0493 - acc: 0.9860 - val_loss: 2.7616 - val_acc: 0.6450\n",
      "Epoch 422/1000\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 0.0542 - acc: 0.9860 - val_loss: 2.7728 - val_acc: 0.6550\n",
      "Epoch 423/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0467 - acc: 0.9870 - val_loss: 2.7821 - val_acc: 0.6550\n",
      "Epoch 424/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0472 - acc: 0.9880 - val_loss: 2.9521 - val_acc: 0.6500\n",
      "Epoch 425/1000\n",
      "1000/1000 [==============================] - 0s 58us/step - loss: 0.0501 - acc: 0.9870 - val_loss: 2.8261 - val_acc: 0.6650\n",
      "Epoch 426/1000\n",
      "1000/1000 [==============================] - 0s 53us/step - loss: 0.0488 - acc: 0.9880 - val_loss: 2.9281 - val_acc: 0.6500\n",
      "Epoch 427/1000\n",
      "1000/1000 [==============================] - 0s 69us/step - loss: 0.0517 - acc: 0.9860 - val_loss: 2.9305 - val_acc: 0.6550\n",
      "Epoch 428/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0549 - acc: 0.9860 - val_loss: 2.9516 - val_acc: 0.6550\n",
      "Epoch 429/1000\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 0.0512 - acc: 0.9860 - val_loss: 2.8999 - val_acc: 0.6550\n",
      "Epoch 430/1000\n",
      "1000/1000 [==============================] - 0s 68us/step - loss: 0.0431 - acc: 0.9880 - val_loss: 2.8668 - val_acc: 0.6550\n",
      "Epoch 431/1000\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 0.0537 - acc: 0.9850 - val_loss: 3.0164 - val_acc: 0.6600\n",
      "Epoch 432/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0433 - acc: 0.9890 - val_loss: 2.9783 - val_acc: 0.6700\n",
      "Epoch 433/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0441 - acc: 0.9880 - val_loss: 2.9968 - val_acc: 0.6650\n",
      "Epoch 434/1000\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.0387 - acc: 0.9900 - val_loss: 3.0195 - val_acc: 0.6600\n",
      "Epoch 435/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0455 - acc: 0.9870 - val_loss: 3.0377 - val_acc: 0.6550\n",
      "Epoch 436/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0432 - acc: 0.9890 - val_loss: 3.0001 - val_acc: 0.6450\n",
      "Epoch 437/1000\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.0373 - acc: 0.9910 - val_loss: 2.9647 - val_acc: 0.6500\n",
      "Epoch 438/1000\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.0409 - acc: 0.9900 - val_loss: 2.9749 - val_acc: 0.6650\n",
      "Epoch 439/1000\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.0469 - acc: 0.9880 - val_loss: 2.9903 - val_acc: 0.6650\n",
      "Epoch 440/1000\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.0379 - acc: 0.9920 - val_loss: 3.0067 - val_acc: 0.6550\n",
      "Epoch 441/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.0413 - acc: 0.9900 - val_loss: 2.9711 - val_acc: 0.6700\n",
      "Epoch 442/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.0435 - acc: 0.9880 - val_loss: 2.9957 - val_acc: 0.6700\n",
      "Epoch 443/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0459 - acc: 0.9860 - val_loss: 2.9981 - val_acc: 0.6650\n",
      "Epoch 444/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0393 - acc: 0.9900 - val_loss: 2.9299 - val_acc: 0.6700\n",
      "Epoch 445/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0375 - acc: 0.9910 - val_loss: 2.9452 - val_acc: 0.6650\n",
      "Epoch 446/1000\n",
      "1000/1000 [==============================] - 0s 56us/step - loss: 0.0458 - acc: 0.9870 - val_loss: 3.0261 - val_acc: 0.6450\n",
      "Epoch 447/1000\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.0437 - acc: 0.9890 - val_loss: 3.0236 - val_acc: 0.6500\n",
      "Epoch 448/1000\n",
      "1000/1000 [==============================] - 0s 58us/step - loss: 0.0403 - acc: 0.9900 - val_loss: 3.0257 - val_acc: 0.6500\n",
      "Epoch 449/1000\n",
      "1000/1000 [==============================] - 0s 56us/step - loss: 0.0425 - acc: 0.9880 - val_loss: 3.0279 - val_acc: 0.6550\n",
      "Epoch 450/1000\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.0460 - acc: 0.9870 - val_loss: 3.0257 - val_acc: 0.6450\n",
      "Epoch 451/1000\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.0437 - acc: 0.9900 - val_loss: 3.0457 - val_acc: 0.6600\n",
      "Epoch 452/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0434 - acc: 0.9880 - val_loss: 3.0150 - val_acc: 0.6450\n",
      "Epoch 453/1000\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 0.0443 - acc: 0.9880 - val_loss: 3.0546 - val_acc: 0.6450\n",
      "Epoch 454/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0432 - acc: 0.9890 - val_loss: 3.0784 - val_acc: 0.6500\n",
      "Epoch 455/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0402 - acc: 0.9910 - val_loss: 3.0596 - val_acc: 0.6550\n",
      "Epoch 456/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.0434 - acc: 0.9890 - val_loss: 3.0695 - val_acc: 0.6550\n",
      "Epoch 457/1000\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 0.0399 - acc: 0.9910 - val_loss: 3.0509 - val_acc: 0.6650\n",
      "Epoch 458/1000\n",
      "1000/1000 [==============================] - 0s 72us/step - loss: 0.0474 - acc: 0.9880 - val_loss: 3.0754 - val_acc: 0.6650\n",
      "Epoch 459/1000\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 0.0449 - acc: 0.9850 - val_loss: 3.0613 - val_acc: 0.6650\n",
      "Epoch 460/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0497 - acc: 0.9870 - val_loss: 3.0485 - val_acc: 0.6650\n",
      "Epoch 461/1000\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 0.0660 - acc: 0.9830 - val_loss: 3.0426 - val_acc: 0.6500\n",
      "Epoch 462/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.1023 - acc: 0.9680 - val_loss: 2.9287 - val_acc: 0.6550\n",
      "Epoch 463/1000\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 0.1514 - acc: 0.9610 - val_loss: 2.7794 - val_acc: 0.6550\n",
      "Epoch 464/1000\n",
      "1000/1000 [==============================] - 0s 79us/step - loss: 0.1609 - acc: 0.9550 - val_loss: 2.7407 - val_acc: 0.6600\n",
      "Epoch 465/1000\n",
      "1000/1000 [==============================] - 0s 69us/step - loss: 0.0828 - acc: 0.9730 - val_loss: 2.9050 - val_acc: 0.6650\n",
      "Epoch 466/1000\n",
      "1000/1000 [==============================] - 0s 67us/step - loss: 0.0709 - acc: 0.9810 - val_loss: 2.8725 - val_acc: 0.6600\n",
      "Epoch 467/1000\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.0566 - acc: 0.9840 - val_loss: 2.8749 - val_acc: 0.6650\n",
      "Epoch 468/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.0544 - acc: 0.9860 - val_loss: 2.9039 - val_acc: 0.6600\n",
      "Epoch 469/1000\n",
      "1000/1000 [==============================] - 0s 72us/step - loss: 0.0792 - acc: 0.9770 - val_loss: 2.8869 - val_acc: 0.6600\n",
      "Epoch 470/1000\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.0653 - acc: 0.9780 - val_loss: 2.8801 - val_acc: 0.6450\n",
      "Epoch 471/1000\n",
      "1000/1000 [==============================] - 0s 67us/step - loss: 0.0525 - acc: 0.9860 - val_loss: 3.0190 - val_acc: 0.6450\n",
      "Epoch 472/1000\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.0511 - acc: 0.9880 - val_loss: 2.9426 - val_acc: 0.6250\n",
      "Epoch 473/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0473 - acc: 0.9900 - val_loss: 2.8946 - val_acc: 0.6500\n",
      "Epoch 474/1000\n",
      "1000/1000 [==============================] - 0s 68us/step - loss: 0.0457 - acc: 0.9890 - val_loss: 2.9594 - val_acc: 0.6450\n",
      "Epoch 475/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.0467 - acc: 0.9870 - val_loss: 2.9660 - val_acc: 0.6500\n",
      "Epoch 476/1000\n",
      "1000/1000 [==============================] - 0s 73us/step - loss: 0.0427 - acc: 0.9890 - val_loss: 2.9752 - val_acc: 0.6450\n",
      "Epoch 477/1000\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.0396 - acc: 0.9900 - val_loss: 3.0338 - val_acc: 0.6500\n",
      "Epoch 478/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.0439 - acc: 0.9900 - val_loss: 3.0077 - val_acc: 0.6450\n",
      "Epoch 479/1000\n",
      "1000/1000 [==============================] - 0s 68us/step - loss: 0.0370 - acc: 0.9920 - val_loss: 3.0271 - val_acc: 0.6450\n",
      "Epoch 480/1000\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 0.0370 - acc: 0.9920 - val_loss: 3.0712 - val_acc: 0.6450\n",
      "Epoch 481/1000\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.0490 - acc: 0.9870 - val_loss: 3.0527 - val_acc: 0.6450\n",
      "Epoch 482/1000\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.0380 - acc: 0.9920 - val_loss: 3.0521 - val_acc: 0.6500\n",
      "Epoch 483/1000\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.0362 - acc: 0.9920 - val_loss: 3.0439 - val_acc: 0.6500\n",
      "Epoch 484/1000\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.0346 - acc: 0.9930 - val_loss: 3.0622 - val_acc: 0.6500\n",
      "Epoch 485/1000\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.0349 - acc: 0.9930 - val_loss: 3.0597 - val_acc: 0.6450\n",
      "Epoch 486/1000\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.0447 - acc: 0.9880 - val_loss: 3.0854 - val_acc: 0.6450\n",
      "Epoch 487/1000\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.0446 - acc: 0.9890 - val_loss: 3.0811 - val_acc: 0.6500\n",
      "Epoch 488/1000\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.0445 - acc: 0.9890 - val_loss: 3.0808 - val_acc: 0.6500\n",
      "Epoch 489/1000\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.0427 - acc: 0.9900 - val_loss: 3.0936 - val_acc: 0.6450\n",
      "Epoch 490/1000\n",
      "1000/1000 [==============================] - 0s 57us/step - loss: 0.0429 - acc: 0.9890 - val_loss: 3.0063 - val_acc: 0.6450\n",
      "Epoch 491/1000\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.0441 - acc: 0.9900 - val_loss: 3.0712 - val_acc: 0.6400\n",
      "Epoch 492/1000\n",
      "1000/1000 [==============================] - 0s 56us/step - loss: 0.0420 - acc: 0.9900 - val_loss: 3.0938 - val_acc: 0.6400\n",
      "Epoch 493/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0396 - acc: 0.9910 - val_loss: 3.0973 - val_acc: 0.6400\n",
      "Epoch 494/1000\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 0.0341 - acc: 0.9930 - val_loss: 3.1018 - val_acc: 0.6350\n",
      "Epoch 495/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0414 - acc: 0.9910 - val_loss: 3.1576 - val_acc: 0.6500\n",
      "Epoch 496/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0418 - acc: 0.9890 - val_loss: 3.0738 - val_acc: 0.6450\n",
      "Epoch 497/1000\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.0432 - acc: 0.9900 - val_loss: 3.1054 - val_acc: 0.6500\n",
      "Epoch 498/1000\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.0489 - acc: 0.9860 - val_loss: 3.0806 - val_acc: 0.6500\n",
      "Epoch 499/1000\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.0581 - acc: 0.9840 - val_loss: 3.0405 - val_acc: 0.6350\n",
      "Epoch 500/1000\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.0457 - acc: 0.9900 - val_loss: 2.9856 - val_acc: 0.6450\n",
      "Epoch 501/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.0486 - acc: 0.9890 - val_loss: 3.0488 - val_acc: 0.6500\n",
      "Epoch 502/1000\n",
      "1000/1000 [==============================] - 0s 80us/step - loss: 0.0481 - acc: 0.9870 - val_loss: 2.9592 - val_acc: 0.6550\n",
      "Epoch 503/1000\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.0879 - acc: 0.9740 - val_loss: 3.1021 - val_acc: 0.6550\n",
      "Epoch 504/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.1143 - acc: 0.9700 - val_loss: 2.9946 - val_acc: 0.6550\n",
      "Epoch 505/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0619 - acc: 0.9820 - val_loss: 3.0037 - val_acc: 0.6650\n",
      "Epoch 506/1000\n",
      "1000/1000 [==============================] - 0s 57us/step - loss: 0.0766 - acc: 0.9790 - val_loss: 2.9616 - val_acc: 0.6700\n",
      "Epoch 507/1000\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 0.0679 - acc: 0.9820 - val_loss: 2.9359 - val_acc: 0.6650\n",
      "Epoch 508/1000\n",
      "1000/1000 [==============================] - 0s 75us/step - loss: 0.0876 - acc: 0.9730 - val_loss: 2.8852 - val_acc: 0.6600\n",
      "Epoch 509/1000\n",
      "1000/1000 [==============================] - 0s 72us/step - loss: 0.0582 - acc: 0.9860 - val_loss: 2.9476 - val_acc: 0.6400\n",
      "Epoch 510/1000\n",
      "1000/1000 [==============================] - 0s 71us/step - loss: 0.0476 - acc: 0.9890 - val_loss: 2.9188 - val_acc: 0.6450\n",
      "Epoch 511/1000\n",
      "1000/1000 [==============================] - 0s 71us/step - loss: 0.0533 - acc: 0.9870 - val_loss: 2.8855 - val_acc: 0.6500\n",
      "Epoch 512/1000\n",
      "1000/1000 [==============================] - 0s 68us/step - loss: 0.0663 - acc: 0.9850 - val_loss: 2.9131 - val_acc: 0.6450\n",
      "Epoch 513/1000\n",
      "1000/1000 [==============================] - 0s 74us/step - loss: 0.0689 - acc: 0.9830 - val_loss: 3.0453 - val_acc: 0.6600\n",
      "Epoch 514/1000\n",
      "1000/1000 [==============================] - 0s 83us/step - loss: 0.0565 - acc: 0.9850 - val_loss: 3.0981 - val_acc: 0.6650\n",
      "Epoch 515/1000\n",
      "1000/1000 [==============================] - 0s 80us/step - loss: 0.0492 - acc: 0.9860 - val_loss: 2.8861 - val_acc: 0.6500\n",
      "Epoch 516/1000\n",
      "1000/1000 [==============================] - 0s 74us/step - loss: 0.0458 - acc: 0.9860 - val_loss: 2.9962 - val_acc: 0.6500\n",
      "Epoch 517/1000\n",
      "1000/1000 [==============================] - 0s 73us/step - loss: 0.0481 - acc: 0.9870 - val_loss: 3.0404 - val_acc: 0.6550\n",
      "Epoch 518/1000\n",
      "1000/1000 [==============================] - 0s 78us/step - loss: 0.0462 - acc: 0.9900 - val_loss: 2.8054 - val_acc: 0.6650\n",
      "Epoch 519/1000\n",
      "1000/1000 [==============================] - 0s 79us/step - loss: 0.0676 - acc: 0.9810 - val_loss: 2.9328 - val_acc: 0.6350\n",
      "Epoch 520/1000\n",
      "1000/1000 [==============================] - 0s 69us/step - loss: 0.0412 - acc: 0.9890 - val_loss: 2.9778 - val_acc: 0.6450\n",
      "Epoch 521/1000\n",
      "1000/1000 [==============================] - 0s 69us/step - loss: 0.0407 - acc: 0.9890 - val_loss: 3.0167 - val_acc: 0.6400\n",
      "Epoch 522/1000\n",
      "1000/1000 [==============================] - 0s 67us/step - loss: 0.0507 - acc: 0.9880 - val_loss: 2.9762 - val_acc: 0.6350\n",
      "Epoch 523/1000\n",
      "1000/1000 [==============================] - 0s 67us/step - loss: 0.0389 - acc: 0.9900 - val_loss: 3.0013 - val_acc: 0.6400\n",
      "Epoch 524/1000\n",
      "1000/1000 [==============================] - 0s 70us/step - loss: 0.0450 - acc: 0.9880 - val_loss: 3.0003 - val_acc: 0.6350\n",
      "Epoch 525/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0406 - acc: 0.9900 - val_loss: 3.0397 - val_acc: 0.6350\n",
      "Epoch 526/1000\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.0367 - acc: 0.9910 - val_loss: 3.0628 - val_acc: 0.6350\n",
      "Epoch 527/1000\n",
      "1000/1000 [==============================] - 0s 73us/step - loss: 0.0429 - acc: 0.9890 - val_loss: 3.0405 - val_acc: 0.6350\n",
      "Epoch 528/1000\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.0387 - acc: 0.9910 - val_loss: 3.1033 - val_acc: 0.6400\n",
      "Epoch 529/1000\n",
      "1000/1000 [==============================] - 0s 68us/step - loss: 0.0439 - acc: 0.9890 - val_loss: 3.0763 - val_acc: 0.6350\n",
      "Epoch 530/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.0382 - acc: 0.9910 - val_loss: 3.0483 - val_acc: 0.6350\n",
      "Epoch 531/1000\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.0392 - acc: 0.9910 - val_loss: 3.0675 - val_acc: 0.6350\n",
      "Epoch 532/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.0366 - acc: 0.9910 - val_loss: 3.0930 - val_acc: 0.6350\n",
      "Epoch 533/1000\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.0392 - acc: 0.9900 - val_loss: 3.1195 - val_acc: 0.6300\n",
      "Epoch 534/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0383 - acc: 0.9910 - val_loss: 3.1507 - val_acc: 0.6350\n",
      "Epoch 535/1000\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 0.0461 - acc: 0.9860 - val_loss: 3.2543 - val_acc: 0.6400\n",
      "Epoch 536/1000\n",
      "1000/1000 [==============================] - 0s 71us/step - loss: 0.0613 - acc: 0.9820 - val_loss: 3.1668 - val_acc: 0.6550\n",
      "Epoch 537/1000\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 0.0454 - acc: 0.9880 - val_loss: 3.1776 - val_acc: 0.6250\n",
      "Epoch 538/1000\n",
      "1000/1000 [==============================] - 0s 76us/step - loss: 0.0480 - acc: 0.9870 - val_loss: 3.0607 - val_acc: 0.6300\n",
      "Epoch 539/1000\n",
      "1000/1000 [==============================] - 0s 71us/step - loss: 0.0450 - acc: 0.9880 - val_loss: 3.1734 - val_acc: 0.6250\n",
      "Epoch 540/1000\n",
      "1000/1000 [==============================] - 0s 73us/step - loss: 0.0356 - acc: 0.9920 - val_loss: 3.1958 - val_acc: 0.6250\n",
      "Epoch 541/1000\n",
      "1000/1000 [==============================] - 0s 73us/step - loss: 0.0474 - acc: 0.9880 - val_loss: 3.2130 - val_acc: 0.6300\n",
      "Epoch 542/1000\n",
      "1000/1000 [==============================] - 0s 71us/step - loss: 0.0375 - acc: 0.9910 - val_loss: 3.2227 - val_acc: 0.6300\n",
      "Epoch 543/1000\n",
      "1000/1000 [==============================] - 0s 73us/step - loss: 0.0430 - acc: 0.9880 - val_loss: 3.0886 - val_acc: 0.6350\n",
      "Epoch 544/1000\n",
      "1000/1000 [==============================] - 0s 73us/step - loss: 0.0479 - acc: 0.9880 - val_loss: 3.2530 - val_acc: 0.6200\n",
      "Epoch 545/1000\n",
      "1000/1000 [==============================] - 0s 72us/step - loss: 0.0358 - acc: 0.9920 - val_loss: 3.2865 - val_acc: 0.6300\n",
      "Epoch 546/1000\n",
      "1000/1000 [==============================] - 0s 77us/step - loss: 0.0496 - acc: 0.9870 - val_loss: 3.3016 - val_acc: 0.6200\n",
      "Epoch 547/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.0368 - acc: 0.9910 - val_loss: 3.2564 - val_acc: 0.6300\n",
      "Epoch 548/1000\n",
      "1000/1000 [==============================] - 0s 69us/step - loss: 0.0331 - acc: 0.9930 - val_loss: 3.2929 - val_acc: 0.6200\n",
      "Epoch 549/1000\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 0.0405 - acc: 0.9900 - val_loss: 3.2946 - val_acc: 0.6250\n",
      "Epoch 550/1000\n",
      "1000/1000 [==============================] - 0s 67us/step - loss: 0.0433 - acc: 0.9890 - val_loss: 3.2111 - val_acc: 0.6300\n",
      "Epoch 551/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0418 - acc: 0.9890 - val_loss: 3.2035 - val_acc: 0.6300\n",
      "Epoch 552/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.0418 - acc: 0.9890 - val_loss: 3.3428 - val_acc: 0.6300\n",
      "Epoch 553/1000\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.0360 - acc: 0.9910 - val_loss: 3.2665 - val_acc: 0.6400\n",
      "Epoch 554/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0434 - acc: 0.9890 - val_loss: 3.3048 - val_acc: 0.6400\n",
      "Epoch 555/1000\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.0384 - acc: 0.9910 - val_loss: 3.2843 - val_acc: 0.6350\n",
      "Epoch 556/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0391 - acc: 0.9900 - val_loss: 3.3236 - val_acc: 0.6350\n",
      "Epoch 557/1000\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 0.0567 - acc: 0.9860 - val_loss: 3.0157 - val_acc: 0.6300\n",
      "Epoch 558/1000\n",
      "1000/1000 [==============================] - 0s 71us/step - loss: 0.0738 - acc: 0.9780 - val_loss: 3.2494 - val_acc: 0.6400\n",
      "Epoch 559/1000\n",
      "1000/1000 [==============================] - 0s 78us/step - loss: 0.1001 - acc: 0.9690 - val_loss: 3.1957 - val_acc: 0.6600\n",
      "Epoch 560/1000\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 0.1001 - acc: 0.9760 - val_loss: 2.9838 - val_acc: 0.6600\n",
      "Epoch 561/1000\n",
      "1000/1000 [==============================] - 0s 68us/step - loss: 0.1241 - acc: 0.9620 - val_loss: 2.9718 - val_acc: 0.6450\n",
      "Epoch 562/1000\n",
      "1000/1000 [==============================] - 0s 70us/step - loss: 0.0659 - acc: 0.9820 - val_loss: 3.0113 - val_acc: 0.6550\n",
      "Epoch 563/1000\n",
      "1000/1000 [==============================] - 0s 72us/step - loss: 0.0428 - acc: 0.9910 - val_loss: 3.0177 - val_acc: 0.6600\n",
      "Epoch 564/1000\n",
      "1000/1000 [==============================] - 0s 81us/step - loss: 0.0427 - acc: 0.9900 - val_loss: 2.9476 - val_acc: 0.6450\n",
      "Epoch 565/1000\n",
      "1000/1000 [==============================] - 0s 70us/step - loss: 0.0410 - acc: 0.9910 - val_loss: 3.0548 - val_acc: 0.6500\n",
      "Epoch 566/1000\n",
      "1000/1000 [==============================] - 0s 76us/step - loss: 0.0419 - acc: 0.9900 - val_loss: 3.0551 - val_acc: 0.6400\n",
      "Epoch 567/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.0394 - acc: 0.9910 - val_loss: 3.0401 - val_acc: 0.6300\n",
      "Epoch 568/1000\n",
      "1000/1000 [==============================] - 0s 57us/step - loss: 0.0399 - acc: 0.9910 - val_loss: 3.1677 - val_acc: 0.6400\n",
      "Epoch 569/1000\n",
      "1000/1000 [==============================] - 0s 92us/step - loss: 0.0437 - acc: 0.9900 - val_loss: 3.1209 - val_acc: 0.6400\n",
      "Epoch 570/1000\n",
      "1000/1000 [==============================] - 0s 120us/step - loss: 0.0440 - acc: 0.9890 - val_loss: 3.2071 - val_acc: 0.6450\n",
      "Epoch 571/1000\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0378 - acc: 0.9910 - val_loss: 3.1912 - val_acc: 0.6500\n",
      "Epoch 572/1000\n",
      "1000/1000 [==============================] - 0s 107us/step - loss: 0.0376 - acc: 0.9910 - val_loss: 3.2057 - val_acc: 0.6450\n",
      "Epoch 573/1000\n",
      "1000/1000 [==============================] - 0s 97us/step - loss: 0.0371 - acc: 0.9910 - val_loss: 3.2027 - val_acc: 0.6500\n",
      "Epoch 574/1000\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0476 - acc: 0.9870 - val_loss: 3.0917 - val_acc: 0.6350\n",
      "Epoch 575/1000\n",
      "1000/1000 [==============================] - 0s 95us/step - loss: 0.0463 - acc: 0.9880 - val_loss: 3.2232 - val_acc: 0.6500\n",
      "Epoch 576/1000\n",
      "1000/1000 [==============================] - 0s 91us/step - loss: 0.0413 - acc: 0.9890 - val_loss: 3.2683 - val_acc: 0.6550\n",
      "Epoch 577/1000\n",
      "1000/1000 [==============================] - 0s 125us/step - loss: 0.0410 - acc: 0.9890 - val_loss: 3.2703 - val_acc: 0.6500\n",
      "Epoch 578/1000\n",
      "1000/1000 [==============================] - 0s 101us/step - loss: 0.0369 - acc: 0.9910 - val_loss: 3.2385 - val_acc: 0.6500\n",
      "Epoch 579/1000\n",
      "1000/1000 [==============================] - 0s 101us/step - loss: 0.0353 - acc: 0.9920 - val_loss: 3.2752 - val_acc: 0.6500\n",
      "Epoch 580/1000\n",
      "1000/1000 [==============================] - 0s 101us/step - loss: 0.0385 - acc: 0.9900 - val_loss: 3.2999 - val_acc: 0.6500\n",
      "Epoch 581/1000\n",
      "1000/1000 [==============================] - 0s 92us/step - loss: 0.0416 - acc: 0.9890 - val_loss: 3.2432 - val_acc: 0.6400\n",
      "Epoch 582/1000\n",
      "1000/1000 [==============================] - 0s 79us/step - loss: 0.0408 - acc: 0.9900 - val_loss: 3.2770 - val_acc: 0.6400\n",
      "Epoch 583/1000\n",
      "1000/1000 [==============================] - 0s 72us/step - loss: 0.0370 - acc: 0.9910 - val_loss: 3.2955 - val_acc: 0.6400\n",
      "Epoch 584/1000\n",
      "1000/1000 [==============================] - 0s 71us/step - loss: 0.0348 - acc: 0.9920 - val_loss: 3.2691 - val_acc: 0.6450\n",
      "Epoch 585/1000\n",
      "1000/1000 [==============================] - 0s 73us/step - loss: 0.0372 - acc: 0.9900 - val_loss: 3.2720 - val_acc: 0.6400\n",
      "Epoch 586/1000\n",
      "1000/1000 [==============================] - 0s 77us/step - loss: 0.0409 - acc: 0.9900 - val_loss: 3.2798 - val_acc: 0.6450\n",
      "Epoch 587/1000\n",
      "1000/1000 [==============================] - 0s 73us/step - loss: 0.0399 - acc: 0.9900 - val_loss: 3.3068 - val_acc: 0.6350\n",
      "Epoch 588/1000\n",
      "1000/1000 [==============================] - 0s 73us/step - loss: 0.0365 - acc: 0.9920 - val_loss: 3.2697 - val_acc: 0.6350\n",
      "Epoch 589/1000\n",
      "1000/1000 [==============================] - 0s 74us/step - loss: 0.0717 - acc: 0.9780 - val_loss: 3.3497 - val_acc: 0.6400\n",
      "Epoch 590/1000\n",
      "1000/1000 [==============================] - 0s 75us/step - loss: 0.1241 - acc: 0.9590 - val_loss: 3.1981 - val_acc: 0.6350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 591/1000\n",
      "1000/1000 [==============================] - 0s 73us/step - loss: 0.0716 - acc: 0.9750 - val_loss: 2.7887 - val_acc: 0.6600\n",
      "Epoch 592/1000\n",
      "1000/1000 [==============================] - 0s 71us/step - loss: 0.0859 - acc: 0.9720 - val_loss: 2.9592 - val_acc: 0.6150\n",
      "Epoch 593/1000\n",
      "1000/1000 [==============================] - 0s 76us/step - loss: 0.0654 - acc: 0.9800 - val_loss: 3.0945 - val_acc: 0.6400\n",
      "Epoch 594/1000\n",
      "1000/1000 [==============================] - 0s 72us/step - loss: 0.0622 - acc: 0.9840 - val_loss: 3.1840 - val_acc: 0.6350\n",
      "Epoch 595/1000\n",
      "1000/1000 [==============================] - 0s 82us/step - loss: 0.0515 - acc: 0.9840 - val_loss: 3.0914 - val_acc: 0.6400\n",
      "Epoch 596/1000\n",
      "1000/1000 [==============================] - 0s 73us/step - loss: 0.0420 - acc: 0.9900 - val_loss: 3.1880 - val_acc: 0.6350\n",
      "Epoch 597/1000\n",
      "1000/1000 [==============================] - 0s 74us/step - loss: 0.0398 - acc: 0.9910 - val_loss: 3.2041 - val_acc: 0.6350\n",
      "Epoch 598/1000\n",
      "1000/1000 [==============================] - 0s 72us/step - loss: 0.0376 - acc: 0.9900 - val_loss: 3.2218 - val_acc: 0.6350\n",
      "Epoch 599/1000\n",
      "1000/1000 [==============================] - 0s 68us/step - loss: 0.0366 - acc: 0.9910 - val_loss: 3.2651 - val_acc: 0.6400\n",
      "Epoch 600/1000\n",
      "1000/1000 [==============================] - 0s 73us/step - loss: 0.0381 - acc: 0.9900 - val_loss: 3.2986 - val_acc: 0.6350\n",
      "Epoch 601/1000\n",
      "1000/1000 [==============================] - 0s 72us/step - loss: 0.0374 - acc: 0.9910 - val_loss: 3.3168 - val_acc: 0.6300\n",
      "Epoch 602/1000\n",
      "1000/1000 [==============================] - 0s 74us/step - loss: 0.0473 - acc: 0.9870 - val_loss: 3.3432 - val_acc: 0.6250\n",
      "Epoch 603/1000\n",
      "1000/1000 [==============================] - 0s 70us/step - loss: 0.0480 - acc: 0.9870 - val_loss: 3.3919 - val_acc: 0.6450\n",
      "Epoch 604/1000\n",
      "1000/1000 [==============================] - 0s 72us/step - loss: 0.0409 - acc: 0.9890 - val_loss: 3.2608 - val_acc: 0.6400\n",
      "Epoch 605/1000\n",
      "1000/1000 [==============================] - 0s 71us/step - loss: 0.0333 - acc: 0.9930 - val_loss: 3.3110 - val_acc: 0.6350\n",
      "Epoch 606/1000\n",
      "1000/1000 [==============================] - 0s 77us/step - loss: 0.0433 - acc: 0.9890 - val_loss: 3.3788 - val_acc: 0.6350\n",
      "Epoch 607/1000\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 0.0392 - acc: 0.9900 - val_loss: 3.3953 - val_acc: 0.6300\n",
      "Epoch 608/1000\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.0383 - acc: 0.9910 - val_loss: 3.4127 - val_acc: 0.6350\n",
      "Epoch 609/1000\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.0357 - acc: 0.9920 - val_loss: 3.3603 - val_acc: 0.6250\n",
      "Epoch 610/1000\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 0.0466 - acc: 0.9870 - val_loss: 3.4601 - val_acc: 0.6350\n",
      "Epoch 611/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.0464 - acc: 0.9870 - val_loss: 3.2396 - val_acc: 0.6350\n",
      "Epoch 612/1000\n",
      "1000/1000 [==============================] - 0s 67us/step - loss: 0.0508 - acc: 0.9870 - val_loss: 3.2567 - val_acc: 0.6450\n",
      "Epoch 613/1000\n",
      "1000/1000 [==============================] - 0s 70us/step - loss: 0.0429 - acc: 0.9880 - val_loss: 3.2385 - val_acc: 0.6300\n",
      "Epoch 614/1000\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.0458 - acc: 0.9880 - val_loss: 3.3506 - val_acc: 0.6300\n",
      "Epoch 615/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.0427 - acc: 0.9890 - val_loss: 3.3134 - val_acc: 0.6450\n",
      "Epoch 616/1000\n",
      "1000/1000 [==============================] - 0s 67us/step - loss: 0.0393 - acc: 0.9900 - val_loss: 3.3011 - val_acc: 0.6400\n",
      "Epoch 617/1000\n",
      "1000/1000 [==============================] - 0s 68us/step - loss: 0.0366 - acc: 0.9910 - val_loss: 3.3646 - val_acc: 0.6400\n",
      "Epoch 618/1000\n",
      "1000/1000 [==============================] - 0s 68us/step - loss: 0.0462 - acc: 0.9880 - val_loss: 3.3691 - val_acc: 0.6400\n",
      "Epoch 619/1000\n",
      "1000/1000 [==============================] - 0s 69us/step - loss: 0.0352 - acc: 0.9910 - val_loss: 3.3664 - val_acc: 0.6500\n",
      "Epoch 620/1000\n",
      "1000/1000 [==============================] - 0s 68us/step - loss: 0.0309 - acc: 0.9930 - val_loss: 3.3829 - val_acc: 0.6450\n",
      "Epoch 621/1000\n",
      "1000/1000 [==============================] - 0s 69us/step - loss: 0.0393 - acc: 0.9890 - val_loss: 3.2812 - val_acc: 0.6400\n",
      "Epoch 622/1000\n",
      "1000/1000 [==============================] - 0s 68us/step - loss: 0.0459 - acc: 0.9880 - val_loss: 3.3160 - val_acc: 0.6400\n",
      "Epoch 623/1000\n",
      "1000/1000 [==============================] - 0s 70us/step - loss: 0.0453 - acc: 0.9860 - val_loss: 3.4329 - val_acc: 0.6600\n",
      "Epoch 624/1000\n",
      "1000/1000 [==============================] - 0s 69us/step - loss: 0.0992 - acc: 0.9690 - val_loss: 3.2585 - val_acc: 0.6500\n",
      "Epoch 625/1000\n",
      "1000/1000 [==============================] - 0s 67us/step - loss: 0.0482 - acc: 0.9890 - val_loss: 3.2126 - val_acc: 0.6450\n",
      "Epoch 626/1000\n",
      "1000/1000 [==============================] - 0s 71us/step - loss: 0.0468 - acc: 0.9880 - val_loss: 3.2512 - val_acc: 0.6450\n",
      "Epoch 627/1000\n",
      "1000/1000 [==============================] - 0s 72us/step - loss: 0.0348 - acc: 0.9920 - val_loss: 3.2543 - val_acc: 0.6400\n",
      "Epoch 628/1000\n",
      "1000/1000 [==============================] - 0s 69us/step - loss: 0.0414 - acc: 0.9900 - val_loss: 3.2760 - val_acc: 0.6400\n",
      "Epoch 629/1000\n",
      "1000/1000 [==============================] - 0s 68us/step - loss: 0.0419 - acc: 0.9890 - val_loss: 3.3006 - val_acc: 0.6450\n",
      "Epoch 630/1000\n",
      "1000/1000 [==============================] - 0s 70us/step - loss: 0.0397 - acc: 0.9910 - val_loss: 3.2592 - val_acc: 0.6350\n",
      "Epoch 631/1000\n",
      "1000/1000 [==============================] - 0s 72us/step - loss: 0.0489 - acc: 0.9860 - val_loss: 3.2931 - val_acc: 0.6300\n",
      "Epoch 632/1000\n",
      "1000/1000 [==============================] - 0s 69us/step - loss: 0.0553 - acc: 0.9870 - val_loss: 3.3004 - val_acc: 0.6450\n",
      "Epoch 633/1000\n",
      "1000/1000 [==============================] - 0s 67us/step - loss: 0.0613 - acc: 0.9840 - val_loss: 3.1765 - val_acc: 0.6250\n",
      "Epoch 634/1000\n",
      "1000/1000 [==============================] - 0s 69us/step - loss: 0.0671 - acc: 0.9810 - val_loss: 3.4268 - val_acc: 0.6450\n",
      "Epoch 635/1000\n",
      "1000/1000 [==============================] - 0s 70us/step - loss: 0.0592 - acc: 0.9830 - val_loss: 3.3396 - val_acc: 0.6500\n",
      "Epoch 636/1000\n",
      "1000/1000 [==============================] - 0s 67us/step - loss: 0.0441 - acc: 0.9890 - val_loss: 3.1355 - val_acc: 0.6350\n",
      "Epoch 637/1000\n",
      "1000/1000 [==============================] - 0s 69us/step - loss: 0.0460 - acc: 0.9880 - val_loss: 3.2017 - val_acc: 0.6450\n",
      "Epoch 638/1000\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 0.0472 - acc: 0.9880 - val_loss: 3.1977 - val_acc: 0.6350\n",
      "Epoch 639/1000\n",
      "1000/1000 [==============================] - 0s 68us/step - loss: 0.0784 - acc: 0.9790 - val_loss: 3.3064 - val_acc: 0.6450\n",
      "Epoch 640/1000\n",
      "1000/1000 [==============================] - 0s 67us/step - loss: 0.0696 - acc: 0.9770 - val_loss: 3.3037 - val_acc: 0.6500\n",
      "Epoch 641/1000\n",
      "1000/1000 [==============================] - 0s 70us/step - loss: 0.1021 - acc: 0.9720 - val_loss: 3.1166 - val_acc: 0.6500\n",
      "Epoch 642/1000\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.1096 - acc: 0.9710 - val_loss: 3.1455 - val_acc: 0.6400\n",
      "Epoch 643/1000\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.0780 - acc: 0.9750 - val_loss: 3.1347 - val_acc: 0.6450\n",
      "Epoch 644/1000\n",
      "1000/1000 [==============================] - 0s 69us/step - loss: 0.0610 - acc: 0.9850 - val_loss: 3.0515 - val_acc: 0.6500\n",
      "Epoch 645/1000\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.0434 - acc: 0.9900 - val_loss: 3.1569 - val_acc: 0.6400\n",
      "Epoch 646/1000\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 0.0428 - acc: 0.9910 - val_loss: 3.1398 - val_acc: 0.6400\n",
      "Epoch 647/1000\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 0.0448 - acc: 0.9890 - val_loss: 3.2536 - val_acc: 0.6400\n",
      "Epoch 648/1000\n",
      "1000/1000 [==============================] - 0s 68us/step - loss: 0.0414 - acc: 0.9900 - val_loss: 3.2550 - val_acc: 0.6400\n",
      "Epoch 649/1000\n",
      "1000/1000 [==============================] - 0s 69us/step - loss: 0.0435 - acc: 0.9890 - val_loss: 3.2980 - val_acc: 0.6500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 650/1000\n",
      "1000/1000 [==============================] - 0s 68us/step - loss: 0.0370 - acc: 0.9920 - val_loss: 3.2587 - val_acc: 0.6450\n",
      "Epoch 651/1000\n",
      "1000/1000 [==============================] - 0s 67us/step - loss: 0.0413 - acc: 0.9880 - val_loss: 3.3017 - val_acc: 0.6400\n",
      "Epoch 652/1000\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.0339 - acc: 0.9920 - val_loss: 3.2872 - val_acc: 0.6400\n",
      "Epoch 653/1000\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 0.0413 - acc: 0.9890 - val_loss: 3.3588 - val_acc: 0.6400\n",
      "Epoch 654/1000\n",
      "1000/1000 [==============================] - 0s 68us/step - loss: 0.0371 - acc: 0.9910 - val_loss: 3.3347 - val_acc: 0.6350\n",
      "Epoch 655/1000\n",
      "1000/1000 [==============================] - 0s 71us/step - loss: 0.0340 - acc: 0.9920 - val_loss: 3.3608 - val_acc: 0.6350\n",
      "Epoch 656/1000\n",
      "1000/1000 [==============================] - 0s 68us/step - loss: 0.0337 - acc: 0.9920 - val_loss: 3.3794 - val_acc: 0.6500\n",
      "Epoch 657/1000\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 0.0430 - acc: 0.9880 - val_loss: 3.3001 - val_acc: 0.6400\n",
      "Epoch 658/1000\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.0327 - acc: 0.9930 - val_loss: 3.3369 - val_acc: 0.6350\n",
      "Epoch 659/1000\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 0.0386 - acc: 0.9910 - val_loss: 3.3708 - val_acc: 0.6400\n",
      "Epoch 660/1000\n",
      "1000/1000 [==============================] - 0s 68us/step - loss: 0.0326 - acc: 0.9920 - val_loss: 3.3752 - val_acc: 0.6400\n",
      "Epoch 661/1000\n",
      "1000/1000 [==============================] - 0s 68us/step - loss: 0.0395 - acc: 0.9900 - val_loss: 3.3551 - val_acc: 0.6350\n",
      "Epoch 662/1000\n",
      "1000/1000 [==============================] - 0s 67us/step - loss: 0.0436 - acc: 0.9880 - val_loss: 3.2533 - val_acc: 0.6300\n",
      "Epoch 663/1000\n",
      "1000/1000 [==============================] - 0s 70us/step - loss: 0.0340 - acc: 0.9920 - val_loss: 3.3045 - val_acc: 0.6350\n",
      "Epoch 664/1000\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 0.0408 - acc: 0.9900 - val_loss: 3.3382 - val_acc: 0.6350\n",
      "Epoch 665/1000\n",
      "1000/1000 [==============================] - 0s 69us/step - loss: 0.0352 - acc: 0.9920 - val_loss: 3.3178 - val_acc: 0.6350\n",
      "Epoch 666/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.0312 - acc: 0.9930 - val_loss: 3.3538 - val_acc: 0.6350\n",
      "Epoch 667/1000\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.0374 - acc: 0.9910 - val_loss: 3.3860 - val_acc: 0.6450\n",
      "Epoch 668/1000\n",
      "1000/1000 [==============================] - 0s 67us/step - loss: 0.0454 - acc: 0.9870 - val_loss: 3.3936 - val_acc: 0.6450\n",
      "Epoch 669/1000\n",
      "1000/1000 [==============================] - 0s 69us/step - loss: 0.0406 - acc: 0.9890 - val_loss: 3.3630 - val_acc: 0.6350\n",
      "Epoch 670/1000\n",
      "1000/1000 [==============================] - 0s 68us/step - loss: 0.0337 - acc: 0.9920 - val_loss: 3.3898 - val_acc: 0.6400\n",
      "Epoch 671/1000\n",
      "1000/1000 [==============================] - 0s 69us/step - loss: 0.0337 - acc: 0.9920 - val_loss: 3.3917 - val_acc: 0.6450\n",
      "Epoch 672/1000\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.0361 - acc: 0.9910 - val_loss: 3.4014 - val_acc: 0.6400\n",
      "Epoch 673/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.0351 - acc: 0.9910 - val_loss: 3.3699 - val_acc: 0.6450\n",
      "Epoch 674/1000\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.0336 - acc: 0.9920 - val_loss: 3.3793 - val_acc: 0.6400\n",
      "Epoch 675/1000\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.0371 - acc: 0.9910 - val_loss: 3.4331 - val_acc: 0.6500\n",
      "Epoch 676/1000\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 0.0377 - acc: 0.9910 - val_loss: 3.4319 - val_acc: 0.6400\n",
      "Epoch 677/1000\n",
      "1000/1000 [==============================] - 0s 68us/step - loss: 0.0402 - acc: 0.9890 - val_loss: 3.4065 - val_acc: 0.6400\n",
      "Epoch 678/1000\n",
      "1000/1000 [==============================] - 0s 67us/step - loss: 0.0388 - acc: 0.9900 - val_loss: 3.3908 - val_acc: 0.6450\n",
      "Epoch 679/1000\n",
      "1000/1000 [==============================] - 0s 71us/step - loss: 0.0390 - acc: 0.9900 - val_loss: 3.4324 - val_acc: 0.6450\n",
      "Epoch 680/1000\n",
      "1000/1000 [==============================] - 0s 68us/step - loss: 0.0377 - acc: 0.9900 - val_loss: 3.3553 - val_acc: 0.6450\n",
      "Epoch 681/1000\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 0.0373 - acc: 0.9910 - val_loss: 3.3736 - val_acc: 0.6450\n",
      "Epoch 682/1000\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 0.0428 - acc: 0.9880 - val_loss: 3.3923 - val_acc: 0.6300\n",
      "Epoch 683/1000\n",
      "1000/1000 [==============================] - 0s 67us/step - loss: 0.0360 - acc: 0.9900 - val_loss: 3.4348 - val_acc: 0.6450\n",
      "Epoch 684/1000\n",
      "1000/1000 [==============================] - 0s 71us/step - loss: 0.0334 - acc: 0.9920 - val_loss: 3.4506 - val_acc: 0.6500\n",
      "Epoch 685/1000\n",
      "1000/1000 [==============================] - 0s 74us/step - loss: 0.0373 - acc: 0.9910 - val_loss: 3.3651 - val_acc: 0.6450\n",
      "Epoch 686/1000\n",
      "1000/1000 [==============================] - 0s 69us/step - loss: 0.0345 - acc: 0.9910 - val_loss: 3.3935 - val_acc: 0.6450\n",
      "Epoch 687/1000\n",
      "1000/1000 [==============================] - 0s 70us/step - loss: 0.0391 - acc: 0.9900 - val_loss: 3.4032 - val_acc: 0.6450\n",
      "Epoch 688/1000\n",
      "1000/1000 [==============================] - 0s 77us/step - loss: 0.0340 - acc: 0.9920 - val_loss: 3.4390 - val_acc: 0.6400\n",
      "Epoch 689/1000\n",
      "1000/1000 [==============================] - 0s 80us/step - loss: 0.0360 - acc: 0.9910 - val_loss: 3.4337 - val_acc: 0.6350\n",
      "Epoch 690/1000\n",
      "1000/1000 [==============================] - 0s 75us/step - loss: 0.0359 - acc: 0.9910 - val_loss: 3.3759 - val_acc: 0.6400\n",
      "Epoch 691/1000\n",
      "1000/1000 [==============================] - 0s 69us/step - loss: 0.0410 - acc: 0.9900 - val_loss: 3.4446 - val_acc: 0.6450\n",
      "Epoch 692/1000\n",
      "1000/1000 [==============================] - 0s 80us/step - loss: 0.0453 - acc: 0.9890 - val_loss: 3.1747 - val_acc: 0.6500\n",
      "Epoch 693/1000\n",
      "1000/1000 [==============================] - 0s 79us/step - loss: 0.0557 - acc: 0.9850 - val_loss: 3.2581 - val_acc: 0.6400\n",
      "Epoch 694/1000\n",
      "1000/1000 [==============================] - 0s 67us/step - loss: 0.1118 - acc: 0.9710 - val_loss: 2.6937 - val_acc: 0.6500\n",
      "Epoch 695/1000\n",
      "1000/1000 [==============================] - 0s 71us/step - loss: 0.1089 - acc: 0.9670 - val_loss: 3.1088 - val_acc: 0.6600\n",
      "Epoch 696/1000\n",
      "1000/1000 [==============================] - 0s 74us/step - loss: 0.1303 - acc: 0.9620 - val_loss: 3.0060 - val_acc: 0.6400\n",
      "Epoch 697/1000\n",
      "1000/1000 [==============================] - 0s 78us/step - loss: 0.1226 - acc: 0.9670 - val_loss: 2.7269 - val_acc: 0.6300\n",
      "Epoch 698/1000\n",
      "1000/1000 [==============================] - 0s 72us/step - loss: 0.0743 - acc: 0.9790 - val_loss: 2.9851 - val_acc: 0.6400\n",
      "Epoch 699/1000\n",
      "1000/1000 [==============================] - 0s 69us/step - loss: 0.0650 - acc: 0.9820 - val_loss: 3.0310 - val_acc: 0.6350\n",
      "Epoch 700/1000\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 0.0530 - acc: 0.9850 - val_loss: 3.0247 - val_acc: 0.6300\n",
      "Epoch 701/1000\n",
      "1000/1000 [==============================] - 0s 77us/step - loss: 0.0455 - acc: 0.9880 - val_loss: 3.1270 - val_acc: 0.6400\n",
      "Epoch 702/1000\n",
      "1000/1000 [==============================] - 0s 78us/step - loss: 0.0393 - acc: 0.9910 - val_loss: 3.1341 - val_acc: 0.6400\n",
      "Epoch 703/1000\n",
      "1000/1000 [==============================] - 0s 67us/step - loss: 0.0362 - acc: 0.9920 - val_loss: 3.2641 - val_acc: 0.6450\n",
      "Epoch 704/1000\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.0418 - acc: 0.9900 - val_loss: 3.2300 - val_acc: 0.6450\n",
      "Epoch 705/1000\n",
      "1000/1000 [==============================] - 0s 70us/step - loss: 0.0440 - acc: 0.9890 - val_loss: 3.2711 - val_acc: 0.6450\n",
      "Epoch 706/1000\n",
      "1000/1000 [==============================] - 0s 74us/step - loss: 0.0407 - acc: 0.9900 - val_loss: 3.2123 - val_acc: 0.6350\n",
      "Epoch 707/1000\n",
      "1000/1000 [==============================] - 0s 81us/step - loss: 0.0345 - acc: 0.9920 - val_loss: 3.2188 - val_acc: 0.6350\n",
      "Epoch 708/1000\n",
      "1000/1000 [==============================] - 0s 71us/step - loss: 0.0388 - acc: 0.9910 - val_loss: 3.2394 - val_acc: 0.6400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 709/1000\n",
      "1000/1000 [==============================] - 0s 67us/step - loss: 0.0382 - acc: 0.9890 - val_loss: 3.2698 - val_acc: 0.6300\n",
      "Epoch 710/1000\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 0.0395 - acc: 0.9890 - val_loss: 3.2700 - val_acc: 0.6300\n",
      "Epoch 711/1000\n",
      "1000/1000 [==============================] - 0s 75us/step - loss: 0.0338 - acc: 0.9920 - val_loss: 3.2751 - val_acc: 0.6350\n",
      "Epoch 712/1000\n",
      "1000/1000 [==============================] - 0s 81us/step - loss: 0.0367 - acc: 0.9910 - val_loss: 3.2849 - val_acc: 0.6350\n",
      "Epoch 713/1000\n",
      "1000/1000 [==============================] - 0s 82us/step - loss: 0.0326 - acc: 0.9920 - val_loss: 3.3155 - val_acc: 0.6350\n",
      "Epoch 714/1000\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 0.0340 - acc: 0.9920 - val_loss: 3.3141 - val_acc: 0.6300\n",
      "Epoch 715/1000\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.0309 - acc: 0.9930 - val_loss: 3.3035 - val_acc: 0.6300\n",
      "Epoch 716/1000\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.0373 - acc: 0.9910 - val_loss: 3.3855 - val_acc: 0.6400\n",
      "Epoch 717/1000\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.0376 - acc: 0.9900 - val_loss: 3.3568 - val_acc: 0.6300\n",
      "Epoch 718/1000\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.0370 - acc: 0.9910 - val_loss: 3.3868 - val_acc: 0.6400\n",
      "Epoch 719/1000\n",
      "1000/1000 [==============================] - 0s 83us/step - loss: 0.0338 - acc: 0.9920 - val_loss: 3.4021 - val_acc: 0.6350\n",
      "Epoch 720/1000\n",
      "1000/1000 [==============================] - 0s 78us/step - loss: 0.0420 - acc: 0.9890 - val_loss: 3.4016 - val_acc: 0.6350\n",
      "Epoch 721/1000\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 0.0377 - acc: 0.9910 - val_loss: 3.3799 - val_acc: 0.6400\n",
      "Epoch 722/1000\n",
      "1000/1000 [==============================] - 0s 68us/step - loss: 0.0394 - acc: 0.9900 - val_loss: 3.3816 - val_acc: 0.6350\n",
      "Epoch 723/1000\n",
      "1000/1000 [==============================] - 0s 70us/step - loss: 0.0394 - acc: 0.9900 - val_loss: 3.3967 - val_acc: 0.6400\n",
      "Epoch 724/1000\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.0395 - acc: 0.9900 - val_loss: 3.4033 - val_acc: 0.6300\n",
      "Epoch 725/1000\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 0.0380 - acc: 0.9900 - val_loss: 3.4449 - val_acc: 0.6350\n",
      "Epoch 726/1000\n",
      "1000/1000 [==============================] - 0s 84us/step - loss: 0.0359 - acc: 0.9910 - val_loss: 3.4456 - val_acc: 0.6350\n",
      "Epoch 727/1000\n",
      "1000/1000 [==============================] - 0s 80us/step - loss: 0.0359 - acc: 0.9910 - val_loss: 3.4363 - val_acc: 0.6450\n",
      "Epoch 728/1000\n",
      "1000/1000 [==============================] - 0s 67us/step - loss: 0.0339 - acc: 0.9910 - val_loss: 3.4361 - val_acc: 0.6400\n",
      "Epoch 729/1000\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.0356 - acc: 0.9910 - val_loss: 3.3669 - val_acc: 0.6350\n",
      "Epoch 730/1000\n",
      "1000/1000 [==============================] - 0s 68us/step - loss: 0.0395 - acc: 0.9890 - val_loss: 3.4494 - val_acc: 0.6400\n",
      "Epoch 731/1000\n",
      "1000/1000 [==============================] - 0s 71us/step - loss: 0.0361 - acc: 0.9900 - val_loss: 3.3270 - val_acc: 0.6350\n",
      "Epoch 732/1000\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.0522 - acc: 0.9830 - val_loss: 3.4214 - val_acc: 0.6450\n",
      "Epoch 733/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.0527 - acc: 0.9850 - val_loss: 3.2457 - val_acc: 0.6500\n",
      "Epoch 734/1000\n",
      "1000/1000 [==============================] - 0s 79us/step - loss: 0.0773 - acc: 0.9790 - val_loss: 3.2556 - val_acc: 0.6400\n",
      "Epoch 735/1000\n",
      "1000/1000 [==============================] - 0s 81us/step - loss: 0.0554 - acc: 0.9840 - val_loss: 3.0966 - val_acc: 0.6500\n",
      "Epoch 736/1000\n",
      "1000/1000 [==============================] - 0s 67us/step - loss: 0.0498 - acc: 0.9860 - val_loss: 3.2521 - val_acc: 0.6500\n",
      "Epoch 737/1000\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 0.0458 - acc: 0.9890 - val_loss: 3.1454 - val_acc: 0.6550\n",
      "Epoch 738/1000\n",
      "1000/1000 [==============================] - 0s 68us/step - loss: 0.0710 - acc: 0.9790 - val_loss: 3.2900 - val_acc: 0.6550\n",
      "Epoch 739/1000\n",
      "1000/1000 [==============================] - 0s 71us/step - loss: 0.0572 - acc: 0.9830 - val_loss: 3.2480 - val_acc: 0.6500\n",
      "Epoch 740/1000\n",
      "1000/1000 [==============================] - 0s 71us/step - loss: 0.0613 - acc: 0.9810 - val_loss: 3.1910 - val_acc: 0.6350\n",
      "Epoch 741/1000\n",
      "1000/1000 [==============================] - 0s 74us/step - loss: 0.0572 - acc: 0.9830 - val_loss: 3.2461 - val_acc: 0.6400\n",
      "Epoch 742/1000\n",
      "1000/1000 [==============================] - 0s 70us/step - loss: 0.0696 - acc: 0.9840 - val_loss: 3.1337 - val_acc: 0.6600\n",
      "Epoch 743/1000\n",
      "1000/1000 [==============================] - 0s 81us/step - loss: 0.0599 - acc: 0.9850 - val_loss: 3.1935 - val_acc: 0.6600\n",
      "Epoch 744/1000\n",
      "1000/1000 [==============================] - 0s 80us/step - loss: 0.0381 - acc: 0.9910 - val_loss: 3.1398 - val_acc: 0.6700\n",
      "Epoch 745/1000\n",
      "1000/1000 [==============================] - 0s 70us/step - loss: 0.0385 - acc: 0.9910 - val_loss: 3.2441 - val_acc: 0.6600\n",
      "Epoch 746/1000\n",
      "1000/1000 [==============================] - 0s 69us/step - loss: 0.0521 - acc: 0.9860 - val_loss: 3.2380 - val_acc: 0.6550\n",
      "Epoch 747/1000\n",
      "1000/1000 [==============================] - 0s 74us/step - loss: 0.0342 - acc: 0.9920 - val_loss: 3.2603 - val_acc: 0.6550\n",
      "Epoch 748/1000\n",
      "1000/1000 [==============================] - 0s 69us/step - loss: 0.0437 - acc: 0.9880 - val_loss: 3.2467 - val_acc: 0.6600\n",
      "Epoch 749/1000\n",
      "1000/1000 [==============================] - 0s 67us/step - loss: 0.0383 - acc: 0.9910 - val_loss: 3.2977 - val_acc: 0.6600\n",
      "Epoch 750/1000\n",
      "1000/1000 [==============================] - 0s 71us/step - loss: 0.0357 - acc: 0.9920 - val_loss: 3.3081 - val_acc: 0.6500\n",
      "Epoch 751/1000\n",
      "1000/1000 [==============================] - 0s 67us/step - loss: 0.0355 - acc: 0.9910 - val_loss: 3.4174 - val_acc: 0.6500\n",
      "Epoch 752/1000\n",
      "1000/1000 [==============================] - 0s 79us/step - loss: 0.0356 - acc: 0.9910 - val_loss: 3.2753 - val_acc: 0.6650\n",
      "Epoch 753/1000\n",
      "1000/1000 [==============================] - 0s 79us/step - loss: 0.0352 - acc: 0.9900 - val_loss: 3.3310 - val_acc: 0.6500\n",
      "Epoch 754/1000\n",
      "1000/1000 [==============================] - 0s 71us/step - loss: 0.0404 - acc: 0.9900 - val_loss: 3.3099 - val_acc: 0.6450\n",
      "Epoch 755/1000\n",
      "1000/1000 [==============================] - 0s 67us/step - loss: 0.0458 - acc: 0.9870 - val_loss: 3.3153 - val_acc: 0.6450\n",
      "Epoch 756/1000\n",
      "1000/1000 [==============================] - 0s 68us/step - loss: 0.0464 - acc: 0.9840 - val_loss: 3.2677 - val_acc: 0.6550\n",
      "Epoch 757/1000\n",
      "1000/1000 [==============================] - 0s 67us/step - loss: 0.0951 - acc: 0.9750 - val_loss: 3.1223 - val_acc: 0.6650\n",
      "Epoch 758/1000\n",
      "1000/1000 [==============================] - 0s 73us/step - loss: 0.0692 - acc: 0.9780 - val_loss: 3.0993 - val_acc: 0.6550\n",
      "Epoch 759/1000\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 0.0571 - acc: 0.9800 - val_loss: 3.1807 - val_acc: 0.6600\n",
      "Epoch 760/1000\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 0.0477 - acc: 0.9880 - val_loss: 3.2827 - val_acc: 0.6500\n",
      "Epoch 761/1000\n",
      "1000/1000 [==============================] - 0s 69us/step - loss: 0.0710 - acc: 0.9830 - val_loss: 3.1444 - val_acc: 0.6600\n",
      "Epoch 762/1000\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 0.0564 - acc: 0.9850 - val_loss: 3.1522 - val_acc: 0.6300\n",
      "Epoch 763/1000\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.0557 - acc: 0.9860 - val_loss: 3.0690 - val_acc: 0.6600\n",
      "Epoch 764/1000\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.0613 - acc: 0.9870 - val_loss: 3.0352 - val_acc: 0.6350\n",
      "Epoch 765/1000\n",
      "1000/1000 [==============================] - 0s 67us/step - loss: 0.0597 - acc: 0.9840 - val_loss: 3.1908 - val_acc: 0.6550\n",
      "Epoch 766/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.0565 - acc: 0.9880 - val_loss: 3.2100 - val_acc: 0.6350\n",
      "Epoch 767/1000\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.0396 - acc: 0.9890 - val_loss: 3.2356 - val_acc: 0.6400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 768/1000\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 0.0427 - acc: 0.9890 - val_loss: 3.1786 - val_acc: 0.6400\n",
      "Epoch 769/1000\n",
      "1000/1000 [==============================] - 0s 70us/step - loss: 0.0372 - acc: 0.9910 - val_loss: 3.2300 - val_acc: 0.6500\n",
      "Epoch 770/1000\n",
      "1000/1000 [==============================] - 0s 70us/step - loss: 0.0341 - acc: 0.9920 - val_loss: 3.2263 - val_acc: 0.6450\n",
      "Epoch 771/1000\n",
      "1000/1000 [==============================] - 0s 67us/step - loss: 0.0319 - acc: 0.9930 - val_loss: 3.2365 - val_acc: 0.6450\n",
      "Epoch 772/1000\n",
      "1000/1000 [==============================] - 0s 73us/step - loss: 0.0320 - acc: 0.9920 - val_loss: 3.2392 - val_acc: 0.6450\n",
      "Epoch 773/1000\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 0.0395 - acc: 0.9900 - val_loss: 3.2518 - val_acc: 0.6450\n",
      "Epoch 774/1000\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.0304 - acc: 0.9930 - val_loss: 3.2381 - val_acc: 0.6350\n",
      "Epoch 775/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.0334 - acc: 0.9920 - val_loss: 3.2688 - val_acc: 0.6450\n",
      "Epoch 776/1000\n",
      "1000/1000 [==============================] - 0s 67us/step - loss: 0.0315 - acc: 0.9930 - val_loss: 3.2351 - val_acc: 0.6350\n",
      "Epoch 777/1000\n",
      "1000/1000 [==============================] - 0s 70us/step - loss: 0.0401 - acc: 0.9890 - val_loss: 3.2593 - val_acc: 0.6400\n",
      "Epoch 778/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.0372 - acc: 0.9900 - val_loss: 3.2400 - val_acc: 0.6450\n",
      "Epoch 779/1000\n",
      "1000/1000 [==============================] - 0s 71us/step - loss: 0.0322 - acc: 0.9920 - val_loss: 3.2705 - val_acc: 0.6450\n",
      "Epoch 780/1000\n",
      "1000/1000 [==============================] - 0s 67us/step - loss: 0.0375 - acc: 0.9900 - val_loss: 3.2954 - val_acc: 0.6450\n",
      "Epoch 781/1000\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.0363 - acc: 0.9900 - val_loss: 3.3033 - val_acc: 0.6450\n",
      "Epoch 782/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.0349 - acc: 0.9910 - val_loss: 3.3185 - val_acc: 0.6400\n",
      "Epoch 783/1000\n",
      "1000/1000 [==============================] - 0s 69us/step - loss: 0.0329 - acc: 0.9920 - val_loss: 3.3170 - val_acc: 0.6450\n",
      "Epoch 784/1000\n",
      "1000/1000 [==============================] - 0s 70us/step - loss: 0.0363 - acc: 0.9910 - val_loss: 3.3089 - val_acc: 0.6450\n",
      "Epoch 785/1000\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.0381 - acc: 0.9900 - val_loss: 3.3346 - val_acc: 0.6500\n",
      "Epoch 786/1000\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 0.0382 - acc: 0.9900 - val_loss: 3.3392 - val_acc: 0.6500\n",
      "Epoch 787/1000\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 0.0322 - acc: 0.9920 - val_loss: 3.3382 - val_acc: 0.6450\n",
      "Epoch 788/1000\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.0301 - acc: 0.9930 - val_loss: 3.3455 - val_acc: 0.6500\n",
      "Epoch 789/1000\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 0.0358 - acc: 0.9910 - val_loss: 3.3738 - val_acc: 0.6500\n",
      "Epoch 790/1000\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 0.0337 - acc: 0.9920 - val_loss: 3.3436 - val_acc: 0.6550\n",
      "Epoch 791/1000\n",
      "1000/1000 [==============================] - 0s 69us/step - loss: 0.0361 - acc: 0.9900 - val_loss: 3.3155 - val_acc: 0.6500\n",
      "Epoch 792/1000\n",
      "1000/1000 [==============================] - 0s 70us/step - loss: 0.0380 - acc: 0.9900 - val_loss: 3.2860 - val_acc: 0.6450\n",
      "Epoch 793/1000\n",
      "1000/1000 [==============================] - 0s 69us/step - loss: 0.0405 - acc: 0.9890 - val_loss: 3.3589 - val_acc: 0.6450\n",
      "Epoch 794/1000\n",
      "1000/1000 [==============================] - 0s 67us/step - loss: 0.0381 - acc: 0.9900 - val_loss: 3.3574 - val_acc: 0.6400\n",
      "Epoch 795/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.0327 - acc: 0.9920 - val_loss: 3.3709 - val_acc: 0.6450\n",
      "Epoch 796/1000\n",
      "1000/1000 [==============================] - 0s 68us/step - loss: 0.0350 - acc: 0.9910 - val_loss: 3.3608 - val_acc: 0.6450\n",
      "Epoch 797/1000\n",
      "1000/1000 [==============================] - 0s 73us/step - loss: 0.0376 - acc: 0.9890 - val_loss: 3.4147 - val_acc: 0.6550\n",
      "Epoch 798/1000\n",
      "1000/1000 [==============================] - 0s 83us/step - loss: 0.0677 - acc: 0.9800 - val_loss: 3.2792 - val_acc: 0.6400\n",
      "Epoch 799/1000\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 0.1002 - acc: 0.9800 - val_loss: 2.8685 - val_acc: 0.6700\n",
      "Epoch 800/1000\n",
      "1000/1000 [==============================] - 0s 58us/step - loss: 0.0818 - acc: 0.9780 - val_loss: 3.2100 - val_acc: 0.6600\n",
      "Epoch 801/1000\n",
      "1000/1000 [==============================] - 0s 77us/step - loss: 0.0596 - acc: 0.9840 - val_loss: 3.1807 - val_acc: 0.6450\n",
      "Epoch 802/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0505 - acc: 0.9870 - val_loss: 3.2678 - val_acc: 0.6450\n",
      "Epoch 803/1000\n",
      "1000/1000 [==============================] - 0s 56us/step - loss: 0.0406 - acc: 0.9910 - val_loss: 3.3005 - val_acc: 0.6450\n",
      "Epoch 804/1000\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 0.0365 - acc: 0.9910 - val_loss: 3.3485 - val_acc: 0.6600\n",
      "Epoch 805/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.0504 - acc: 0.9890 - val_loss: 3.4602 - val_acc: 0.6500\n",
      "Epoch 806/1000\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.0567 - acc: 0.9840 - val_loss: 3.3886 - val_acc: 0.6500\n",
      "Epoch 807/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.0459 - acc: 0.9860 - val_loss: 3.2985 - val_acc: 0.6550\n",
      "Epoch 808/1000\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.0376 - acc: 0.9900 - val_loss: 3.2661 - val_acc: 0.6500\n",
      "Epoch 809/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0397 - acc: 0.9890 - val_loss: 3.2155 - val_acc: 0.6500\n",
      "Epoch 810/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0387 - acc: 0.9910 - val_loss: 3.2696 - val_acc: 0.6600\n",
      "Epoch 811/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0386 - acc: 0.9900 - val_loss: 3.2866 - val_acc: 0.6500\n",
      "Epoch 812/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0308 - acc: 0.9930 - val_loss: 3.3072 - val_acc: 0.6550\n",
      "Epoch 813/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0362 - acc: 0.9910 - val_loss: 3.3029 - val_acc: 0.6500\n",
      "Epoch 814/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.0278 - acc: 0.9940 - val_loss: 3.3164 - val_acc: 0.6450\n",
      "Epoch 815/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0271 - acc: 0.9940 - val_loss: 3.3228 - val_acc: 0.6450\n",
      "Epoch 816/1000\n",
      "1000/1000 [==============================] - 0s 67us/step - loss: 0.0327 - acc: 0.9920 - val_loss: 3.3527 - val_acc: 0.6400\n",
      "Epoch 817/1000\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.0304 - acc: 0.9930 - val_loss: 3.3665 - val_acc: 0.6550\n",
      "Epoch 818/1000\n",
      "1000/1000 [==============================] - 0s 57us/step - loss: 0.0298 - acc: 0.9930 - val_loss: 3.3612 - val_acc: 0.6500\n",
      "Epoch 819/1000\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 0.0268 - acc: 0.9940 - val_loss: 3.3602 - val_acc: 0.6500\n",
      "Epoch 820/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.0295 - acc: 0.9930 - val_loss: 3.3761 - val_acc: 0.6500\n",
      "Epoch 821/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.0332 - acc: 0.9910 - val_loss: 3.3444 - val_acc: 0.6550\n",
      "Epoch 822/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0293 - acc: 0.9930 - val_loss: 3.3636 - val_acc: 0.6500\n",
      "Epoch 823/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0266 - acc: 0.9940 - val_loss: 3.3628 - val_acc: 0.6550\n",
      "Epoch 824/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0295 - acc: 0.9930 - val_loss: 3.4102 - val_acc: 0.6500\n",
      "Epoch 825/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0325 - acc: 0.9920 - val_loss: 3.4167 - val_acc: 0.6450\n",
      "Epoch 826/1000\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.0395 - acc: 0.9900 - val_loss: 3.4116 - val_acc: 0.6500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 827/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0275 - acc: 0.9940 - val_loss: 3.4232 - val_acc: 0.6500\n",
      "Epoch 828/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0316 - acc: 0.9920 - val_loss: 3.3891 - val_acc: 0.6550\n",
      "Epoch 829/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0418 - acc: 0.9890 - val_loss: 3.4485 - val_acc: 0.6500\n",
      "Epoch 830/1000\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.0346 - acc: 0.9900 - val_loss: 3.3471 - val_acc: 0.6500\n",
      "Epoch 831/1000\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 0.0794 - acc: 0.9800 - val_loss: 3.1561 - val_acc: 0.6400\n",
      "Epoch 832/1000\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 0.0676 - acc: 0.9810 - val_loss: 3.1091 - val_acc: 0.6600\n",
      "Epoch 833/1000\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.0900 - acc: 0.9720 - val_loss: 3.2252 - val_acc: 0.6700\n",
      "Epoch 834/1000\n",
      "1000/1000 [==============================] - 0s 72us/step - loss: 0.0625 - acc: 0.9800 - val_loss: 3.1226 - val_acc: 0.6600\n",
      "Epoch 835/1000\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 0.0365 - acc: 0.9900 - val_loss: 3.2650 - val_acc: 0.6650\n",
      "Epoch 836/1000\n",
      "1000/1000 [==============================] - 0s 73us/step - loss: 0.0312 - acc: 0.9930 - val_loss: 3.3081 - val_acc: 0.6600\n",
      "Epoch 837/1000\n",
      "1000/1000 [==============================] - 0s 67us/step - loss: 0.0358 - acc: 0.9910 - val_loss: 3.3487 - val_acc: 0.6600\n",
      "Epoch 838/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0276 - acc: 0.9940 - val_loss: 3.3516 - val_acc: 0.6500\n",
      "Epoch 839/1000\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.0304 - acc: 0.9930 - val_loss: 3.3729 - val_acc: 0.6500\n",
      "Epoch 840/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0305 - acc: 0.9920 - val_loss: 3.4072 - val_acc: 0.6500\n",
      "Epoch 841/1000\n",
      "1000/1000 [==============================] - 0s 151us/step - loss: 0.0308 - acc: 0.9930 - val_loss: 3.3988 - val_acc: 0.6500\n",
      "Epoch 842/1000\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 0.0310 - acc: 0.9930 - val_loss: 3.4483 - val_acc: 0.6500\n",
      "Epoch 843/1000\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 0.0355 - acc: 0.9910 - val_loss: 3.4288 - val_acc: 0.6500\n",
      "Epoch 844/1000\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.0332 - acc: 0.9920 - val_loss: 3.4471 - val_acc: 0.6600\n",
      "Epoch 845/1000\n",
      "1000/1000 [==============================] - 0s 51us/step - loss: 0.0324 - acc: 0.9920 - val_loss: 3.4526 - val_acc: 0.6500\n",
      "Epoch 846/1000\n",
      "1000/1000 [==============================] - 0s 75us/step - loss: 0.0321 - acc: 0.9920 - val_loss: 3.4191 - val_acc: 0.6500\n",
      "Epoch 847/1000\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.0286 - acc: 0.9930 - val_loss: 3.4279 - val_acc: 0.6500\n",
      "Epoch 848/1000\n",
      "1000/1000 [==============================] - 0s 69us/step - loss: 0.0298 - acc: 0.9930 - val_loss: 3.4349 - val_acc: 0.6500\n",
      "Epoch 849/1000\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 0.0295 - acc: 0.9930 - val_loss: 3.4134 - val_acc: 0.6450\n",
      "Epoch 850/1000\n",
      "1000/1000 [==============================] - 0s 68us/step - loss: 0.0321 - acc: 0.9920 - val_loss: 3.4389 - val_acc: 0.6550\n",
      "Epoch 851/1000\n",
      "1000/1000 [==============================] - 0s 82us/step - loss: 0.0285 - acc: 0.9930 - val_loss: 3.4361 - val_acc: 0.6500\n",
      "Epoch 852/1000\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.0314 - acc: 0.9930 - val_loss: 3.4673 - val_acc: 0.6550\n",
      "Epoch 853/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0282 - acc: 0.9930 - val_loss: 3.4604 - val_acc: 0.6550\n",
      "Epoch 854/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0338 - acc: 0.9920 - val_loss: 3.4936 - val_acc: 0.6600\n",
      "Epoch 855/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0301 - acc: 0.9930 - val_loss: 3.4974 - val_acc: 0.6500\n",
      "Epoch 856/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0295 - acc: 0.9930 - val_loss: 3.4664 - val_acc: 0.6600\n",
      "Epoch 857/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0329 - acc: 0.9920 - val_loss: 3.4897 - val_acc: 0.6600\n",
      "Epoch 858/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0520 - acc: 0.9830 - val_loss: 3.3971 - val_acc: 0.6700\n",
      "Epoch 859/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0786 - acc: 0.9790 - val_loss: 3.2517 - val_acc: 0.6300\n",
      "Epoch 860/1000\n",
      "1000/1000 [==============================] - 0s 57us/step - loss: 0.1361 - acc: 0.9640 - val_loss: 3.3107 - val_acc: 0.6550\n",
      "Epoch 861/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.1337 - acc: 0.9650 - val_loss: 3.4157 - val_acc: 0.6450\n",
      "Epoch 862/1000\n",
      "1000/1000 [==============================] - 0s 55us/step - loss: 0.0587 - acc: 0.9860 - val_loss: 3.2280 - val_acc: 0.6400\n",
      "Epoch 863/1000\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 0.0326 - acc: 0.9930 - val_loss: 3.2418 - val_acc: 0.6500\n",
      "Epoch 864/1000\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 0.0369 - acc: 0.9900 - val_loss: 3.2844 - val_acc: 0.6500\n",
      "Epoch 865/1000\n",
      "1000/1000 [==============================] - 0s 58us/step - loss: 0.0423 - acc: 0.9890 - val_loss: 3.3332 - val_acc: 0.6550\n",
      "Epoch 866/1000\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 0.0394 - acc: 0.9910 - val_loss: 3.3581 - val_acc: 0.6500\n",
      "Epoch 867/1000\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.0363 - acc: 0.9910 - val_loss: 3.3528 - val_acc: 0.6550\n",
      "Epoch 868/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0304 - acc: 0.9930 - val_loss: 3.3504 - val_acc: 0.6500\n",
      "Epoch 869/1000\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.0386 - acc: 0.9900 - val_loss: 3.3620 - val_acc: 0.6550\n",
      "Epoch 870/1000\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 0.0366 - acc: 0.9910 - val_loss: 3.3613 - val_acc: 0.6600\n",
      "Epoch 871/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0296 - acc: 0.9930 - val_loss: 3.3545 - val_acc: 0.6500\n",
      "Epoch 872/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0323 - acc: 0.9920 - val_loss: 3.3601 - val_acc: 0.6450\n",
      "Epoch 873/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0358 - acc: 0.9910 - val_loss: 3.3800 - val_acc: 0.6600\n",
      "Epoch 874/1000\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.0264 - acc: 0.9940 - val_loss: 3.4044 - val_acc: 0.6600\n",
      "Epoch 875/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0286 - acc: 0.9930 - val_loss: 3.4190 - val_acc: 0.6550\n",
      "Epoch 876/1000\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.0281 - acc: 0.9930 - val_loss: 3.4393 - val_acc: 0.6500\n",
      "Epoch 877/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0322 - acc: 0.9920 - val_loss: 3.4545 - val_acc: 0.6400\n",
      "Epoch 878/1000\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 0.0324 - acc: 0.9920 - val_loss: 3.4787 - val_acc: 0.6500\n",
      "Epoch 879/1000\n",
      "1000/1000 [==============================] - 0s 72us/step - loss: 0.0255 - acc: 0.9940 - val_loss: 3.4892 - val_acc: 0.6550\n",
      "Epoch 880/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0381 - acc: 0.9890 - val_loss: 3.4739 - val_acc: 0.6500\n",
      "Epoch 881/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0490 - acc: 0.9910 - val_loss: 3.4156 - val_acc: 0.6350\n",
      "Epoch 882/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0530 - acc: 0.9860 - val_loss: 3.3819 - val_acc: 0.6350\n",
      "Epoch 883/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0369 - acc: 0.9900 - val_loss: 3.4148 - val_acc: 0.6550\n",
      "Epoch 884/1000\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.0313 - acc: 0.9930 - val_loss: 3.4008 - val_acc: 0.6550\n",
      "Epoch 885/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0300 - acc: 0.9930 - val_loss: 3.3964 - val_acc: 0.6550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 886/1000\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.0319 - acc: 0.9920 - val_loss: 3.3732 - val_acc: 0.6350\n",
      "Epoch 887/1000\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.0472 - acc: 0.9880 - val_loss: 3.4036 - val_acc: 0.6550\n",
      "Epoch 888/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0329 - acc: 0.9920 - val_loss: 3.3714 - val_acc: 0.6400\n",
      "Epoch 889/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0325 - acc: 0.9920 - val_loss: 3.3835 - val_acc: 0.6350\n",
      "Epoch 890/1000\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.0324 - acc: 0.9920 - val_loss: 3.4029 - val_acc: 0.6450\n",
      "Epoch 891/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0323 - acc: 0.9920 - val_loss: 3.4295 - val_acc: 0.6500\n",
      "Epoch 892/1000\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.0285 - acc: 0.9930 - val_loss: 3.4064 - val_acc: 0.6350\n",
      "Epoch 893/1000\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 0.0381 - acc: 0.9900 - val_loss: 3.4668 - val_acc: 0.6550\n",
      "Epoch 894/1000\n",
      "1000/1000 [==============================] - 0s 69us/step - loss: 0.0370 - acc: 0.9910 - val_loss: 3.3584 - val_acc: 0.6350\n",
      "Epoch 895/1000\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.0458 - acc: 0.9890 - val_loss: 3.4444 - val_acc: 0.6550\n",
      "Epoch 896/1000\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 0.0495 - acc: 0.9860 - val_loss: 3.5108 - val_acc: 0.6550\n",
      "Epoch 897/1000\n",
      "1000/1000 [==============================] - 0s 74us/step - loss: 0.0816 - acc: 0.9790 - val_loss: 3.5002 - val_acc: 0.6600\n",
      "Epoch 898/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0830 - acc: 0.9730 - val_loss: 3.2256 - val_acc: 0.6600\n",
      "Epoch 899/1000\n",
      "1000/1000 [==============================] - 0s 53us/step - loss: 0.0797 - acc: 0.9760 - val_loss: 3.1499 - val_acc: 0.6500\n",
      "Epoch 900/1000\n",
      "1000/1000 [==============================] - 0s 69us/step - loss: 0.0759 - acc: 0.9760 - val_loss: 3.3430 - val_acc: 0.6700\n",
      "Epoch 901/1000\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.0683 - acc: 0.9810 - val_loss: 3.1427 - val_acc: 0.6350\n",
      "Epoch 902/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0539 - acc: 0.9830 - val_loss: 3.3214 - val_acc: 0.6400\n",
      "Epoch 903/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0489 - acc: 0.9860 - val_loss: 3.2185 - val_acc: 0.6500\n",
      "Epoch 904/1000\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.0804 - acc: 0.9790 - val_loss: 3.1475 - val_acc: 0.6600\n",
      "Epoch 905/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0885 - acc: 0.9750 - val_loss: 3.1854 - val_acc: 0.6600\n",
      "Epoch 906/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0424 - acc: 0.9870 - val_loss: 3.1688 - val_acc: 0.6500\n",
      "Epoch 907/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0415 - acc: 0.9910 - val_loss: 3.2066 - val_acc: 0.6500\n",
      "Epoch 908/1000\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 0.0285 - acc: 0.9930 - val_loss: 3.3168 - val_acc: 0.6550\n",
      "Epoch 909/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0354 - acc: 0.9900 - val_loss: 3.1847 - val_acc: 0.6500\n",
      "Epoch 910/1000\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 0.0337 - acc: 0.9920 - val_loss: 3.2682 - val_acc: 0.6500\n",
      "Epoch 911/1000\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 0.0308 - acc: 0.9930 - val_loss: 3.2838 - val_acc: 0.6500\n",
      "Epoch 912/1000\n",
      "1000/1000 [==============================] - 0s 68us/step - loss: 0.0306 - acc: 0.9930 - val_loss: 3.3174 - val_acc: 0.6550\n",
      "Epoch 913/1000\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 0.0335 - acc: 0.9920 - val_loss: 3.3523 - val_acc: 0.6550\n",
      "Epoch 914/1000\n",
      "1000/1000 [==============================] - 0s 71us/step - loss: 0.0362 - acc: 0.9910 - val_loss: 3.3596 - val_acc: 0.6600\n",
      "Epoch 915/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0287 - acc: 0.9920 - val_loss: 3.4088 - val_acc: 0.6550\n",
      "Epoch 916/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.0314 - acc: 0.9920 - val_loss: 3.4116 - val_acc: 0.6550\n",
      "Epoch 917/1000\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.0232 - acc: 0.9950 - val_loss: 3.4277 - val_acc: 0.6550\n",
      "Epoch 918/1000\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.0341 - acc: 0.9900 - val_loss: 3.4140 - val_acc: 0.6550\n",
      "Epoch 919/1000\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.0329 - acc: 0.9920 - val_loss: 3.4534 - val_acc: 0.6550\n",
      "Epoch 920/1000\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.0288 - acc: 0.9930 - val_loss: 3.4551 - val_acc: 0.6550\n",
      "Epoch 921/1000\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.0241 - acc: 0.9950 - val_loss: 3.4468 - val_acc: 0.6550\n",
      "Epoch 922/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0308 - acc: 0.9930 - val_loss: 3.4592 - val_acc: 0.6550\n",
      "Epoch 923/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0361 - acc: 0.9910 - val_loss: 3.4619 - val_acc: 0.6550\n",
      "Epoch 924/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0352 - acc: 0.9910 - val_loss: 3.4801 - val_acc: 0.6500\n",
      "Epoch 925/1000\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.0325 - acc: 0.9920 - val_loss: 3.4863 - val_acc: 0.6550\n",
      "Epoch 926/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0383 - acc: 0.9900 - val_loss: 3.5081 - val_acc: 0.6550\n",
      "Epoch 927/1000\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 0.0313 - acc: 0.9920 - val_loss: 3.4774 - val_acc: 0.6600\n",
      "Epoch 928/1000\n",
      "1000/1000 [==============================] - 0s 74us/step - loss: 0.0292 - acc: 0.9920 - val_loss: 3.4804 - val_acc: 0.6550\n",
      "Epoch 929/1000\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 0.0227 - acc: 0.9950 - val_loss: 3.4691 - val_acc: 0.6550\n",
      "Epoch 930/1000\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 0.0266 - acc: 0.9930 - val_loss: 3.4862 - val_acc: 0.6550\n",
      "Epoch 931/1000\n",
      "1000/1000 [==============================] - 0s 72us/step - loss: 0.0320 - acc: 0.9920 - val_loss: 3.5328 - val_acc: 0.6600\n",
      "Epoch 932/1000\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 0.0271 - acc: 0.9940 - val_loss: 3.4624 - val_acc: 0.6600\n",
      "Epoch 933/1000\n",
      "1000/1000 [==============================] - 0s 77us/step - loss: 0.0306 - acc: 0.9920 - val_loss: 3.4873 - val_acc: 0.6650\n",
      "Epoch 934/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0235 - acc: 0.9950 - val_loss: 3.4891 - val_acc: 0.6600\n",
      "Epoch 935/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0218 - acc: 0.9950 - val_loss: 3.4981 - val_acc: 0.6600\n",
      "Epoch 936/1000\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.0346 - acc: 0.9910 - val_loss: 3.3956 - val_acc: 0.6500\n",
      "Epoch 937/1000\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.0435 - acc: 0.9900 - val_loss: 3.3514 - val_acc: 0.6350\n",
      "Epoch 938/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0659 - acc: 0.9800 - val_loss: 3.4752 - val_acc: 0.6600\n",
      "Epoch 939/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.0489 - acc: 0.9850 - val_loss: 3.4443 - val_acc: 0.6400\n",
      "Epoch 940/1000\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 0.0405 - acc: 0.9900 - val_loss: 3.2078 - val_acc: 0.6600\n",
      "Epoch 941/1000\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.0642 - acc: 0.9830 - val_loss: 3.4537 - val_acc: 0.6550\n",
      "Epoch 942/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0450 - acc: 0.9870 - val_loss: 3.1297 - val_acc: 0.6450\n",
      "Epoch 943/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.0589 - acc: 0.9840 - val_loss: 3.1369 - val_acc: 0.6450\n",
      "Epoch 944/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0407 - acc: 0.9900 - val_loss: 3.2903 - val_acc: 0.6450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 945/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0360 - acc: 0.9900 - val_loss: 3.3644 - val_acc: 0.6500\n",
      "Epoch 946/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0319 - acc: 0.9920 - val_loss: 3.3692 - val_acc: 0.6450\n",
      "Epoch 947/1000\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 0.0265 - acc: 0.9940 - val_loss: 3.3751 - val_acc: 0.6450\n",
      "Epoch 948/1000\n",
      "1000/1000 [==============================] - 0s 57us/step - loss: 0.0285 - acc: 0.9930 - val_loss: 3.3682 - val_acc: 0.6450\n",
      "Epoch 949/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0299 - acc: 0.9930 - val_loss: 3.3883 - val_acc: 0.6500\n",
      "Epoch 950/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0303 - acc: 0.9930 - val_loss: 3.3811 - val_acc: 0.6500\n",
      "Epoch 951/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0287 - acc: 0.9930 - val_loss: 3.3889 - val_acc: 0.6500\n",
      "Epoch 952/1000\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 0.0251 - acc: 0.9940 - val_loss: 3.4164 - val_acc: 0.6500\n",
      "Epoch 953/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0350 - acc: 0.9910 - val_loss: 3.4021 - val_acc: 0.6450\n",
      "Epoch 954/1000\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 0.0319 - acc: 0.9910 - val_loss: 3.4780 - val_acc: 0.6450\n",
      "Epoch 955/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0258 - acc: 0.9940 - val_loss: 3.4267 - val_acc: 0.6550\n",
      "Epoch 956/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0349 - acc: 0.9910 - val_loss: 3.4480 - val_acc: 0.6500\n",
      "Epoch 957/1000\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.0267 - acc: 0.9930 - val_loss: 3.4645 - val_acc: 0.6650\n",
      "Epoch 958/1000\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 0.0301 - acc: 0.9930 - val_loss: 3.4046 - val_acc: 0.6400\n",
      "Epoch 959/1000\n",
      "1000/1000 [==============================] - 0s 69us/step - loss: 0.0278 - acc: 0.9930 - val_loss: 3.4331 - val_acc: 0.6550\n",
      "Epoch 960/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0304 - acc: 0.9910 - val_loss: 3.4490 - val_acc: 0.6550\n",
      "Epoch 961/1000\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.0348 - acc: 0.9910 - val_loss: 3.4482 - val_acc: 0.6550\n",
      "Epoch 962/1000\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 0.0259 - acc: 0.9930 - val_loss: 3.4387 - val_acc: 0.6450\n",
      "Epoch 963/1000\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 0.0422 - acc: 0.9910 - val_loss: 3.4478 - val_acc: 0.6500\n",
      "Epoch 964/1000\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 0.0551 - acc: 0.9840 - val_loss: 3.5013 - val_acc: 0.6600\n",
      "Epoch 965/1000\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.1140 - acc: 0.9710 - val_loss: 3.1864 - val_acc: 0.6500\n",
      "Epoch 966/1000\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.0964 - acc: 0.9750 - val_loss: 3.3636 - val_acc: 0.6600\n",
      "Epoch 967/1000\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.0472 - acc: 0.9850 - val_loss: 3.2820 - val_acc: 0.6500\n",
      "Epoch 968/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0279 - acc: 0.9940 - val_loss: 3.3299 - val_acc: 0.6500\n",
      "Epoch 969/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0307 - acc: 0.9920 - val_loss: 3.2740 - val_acc: 0.6600\n",
      "Epoch 970/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0331 - acc: 0.9920 - val_loss: 3.2898 - val_acc: 0.6550\n",
      "Epoch 971/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0348 - acc: 0.9910 - val_loss: 3.3265 - val_acc: 0.6550\n",
      "Epoch 972/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0338 - acc: 0.9910 - val_loss: 3.3989 - val_acc: 0.6600\n",
      "Epoch 973/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0285 - acc: 0.9940 - val_loss: 3.4053 - val_acc: 0.6500\n",
      "Epoch 974/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0332 - acc: 0.9910 - val_loss: 3.4585 - val_acc: 0.6500\n",
      "Epoch 975/1000\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 0.0238 - acc: 0.9950 - val_loss: 3.4355 - val_acc: 0.6600\n",
      "Epoch 976/1000\n",
      "1000/1000 [==============================] - 0s 77us/step - loss: 0.0264 - acc: 0.9940 - val_loss: 3.4424 - val_acc: 0.6550\n",
      "Epoch 977/1000\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 0.0319 - acc: 0.9910 - val_loss: 3.4501 - val_acc: 0.6550\n",
      "Epoch 978/1000\n",
      "1000/1000 [==============================] - 0s 70us/step - loss: 0.0355 - acc: 0.9910 - val_loss: 3.4646 - val_acc: 0.6550\n",
      "Epoch 979/1000\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 0.0268 - acc: 0.9930 - val_loss: 3.4775 - val_acc: 0.6550\n",
      "Epoch 980/1000\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 0.0301 - acc: 0.9920 - val_loss: 3.4509 - val_acc: 0.6450\n",
      "Epoch 981/1000\n",
      "1000/1000 [==============================] - 0s 77us/step - loss: 0.0291 - acc: 0.9920 - val_loss: 3.5091 - val_acc: 0.6550\n",
      "Epoch 982/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0329 - acc: 0.9920 - val_loss: 3.4706 - val_acc: 0.6400\n",
      "Epoch 983/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0263 - acc: 0.9940 - val_loss: 3.4804 - val_acc: 0.6350\n",
      "Epoch 984/1000\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.0260 - acc: 0.9940 - val_loss: 3.4766 - val_acc: 0.6500\n",
      "Epoch 985/1000\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.0319 - acc: 0.9920 - val_loss: 3.5025 - val_acc: 0.6500\n",
      "Epoch 986/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0383 - acc: 0.9900 - val_loss: 3.5132 - val_acc: 0.6650\n",
      "Epoch 987/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0314 - acc: 0.9920 - val_loss: 3.5092 - val_acc: 0.6300\n",
      "Epoch 988/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0295 - acc: 0.9930 - val_loss: 3.5157 - val_acc: 0.6350\n",
      "Epoch 989/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0319 - acc: 0.9920 - val_loss: 3.4962 - val_acc: 0.6400\n",
      "Epoch 990/1000\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.0294 - acc: 0.9930 - val_loss: 3.5078 - val_acc: 0.6400\n",
      "Epoch 991/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0272 - acc: 0.9940 - val_loss: 3.5261 - val_acc: 0.6400\n",
      "Epoch 992/1000\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.0229 - acc: 0.9950 - val_loss: 3.5657 - val_acc: 0.6350\n",
      "Epoch 993/1000\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 0.0371 - acc: 0.9900 - val_loss: 3.5809 - val_acc: 0.6400\n",
      "Epoch 994/1000\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 0.0321 - acc: 0.9920 - val_loss: 3.5737 - val_acc: 0.6500\n",
      "Epoch 995/1000\n",
      "1000/1000 [==============================] - 0s 72us/step - loss: 0.0276 - acc: 0.9930 - val_loss: 3.5788 - val_acc: 0.6450\n",
      "Epoch 996/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0341 - acc: 0.9910 - val_loss: 3.5871 - val_acc: 0.6450\n",
      "Epoch 997/1000\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.0318 - acc: 0.9920 - val_loss: 3.6086 - val_acc: 0.6450\n",
      "Epoch 998/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0338 - acc: 0.9910 - val_loss: 3.6200 - val_acc: 0.6400\n",
      "Epoch 999/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0220 - acc: 0.9950 - val_loss: 3.6249 - val_acc: 0.6400\n",
      "Epoch 1000/1000\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0289 - acc: 0.9920 - val_loss: 3.6169 - val_acc: 0.6550\n"
     ]
    }
   ],
   "source": [
    "# call the function to fit to the data (training the network)\n",
    "history=model.fit(X_train, y_train, epochs = 1000, batch_size=20, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_keras = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.2701071e-03],\n",
       "       [9.9997878e-01],\n",
       "       [9.9996877e-01],\n",
       "       [3.1993326e-04],\n",
       "       [1.0000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_keras[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert in crisp class\n",
    "yhat_class_keras = model.predict_classes(X_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6703296703296703\n",
      "0.655\n"
     ]
    }
   ],
   "source": [
    "print(recall_score(y_test, yhat_class_keras))\n",
    "print(accuracy_score(y_test, yhat_class_keras))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_score = recall_score(y_test, yhat_class_keras)\n",
    "keras_acc = accuracy_score(y_test, yhat_class_keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.64      0.67       109\n",
      "           1       0.61      0.67      0.64        91\n",
      "\n",
      "   micro avg       0.66      0.66      0.66       200\n",
      "   macro avg       0.66      0.66      0.65       200\n",
      "weighted avg       0.66      0.66      0.66       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(classification_report(y_test, yhat_class_keras))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def plot_history(history):\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_history(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
